{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredHTMLLoader as HTMLLoader\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders():\n",
    "    files = glob(\"pl-docs/*.html\")\n",
    "    return [HTMLLoader(file) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents():\n",
    "    documents = []\n",
    "    ids = []\n",
    "    files = glob(\"pl-docs/*.html\")\n",
    "    for file in tqdm(files):\n",
    "        loader = HTMLLoader(file)\n",
    "        document = loader.load()\n",
    "        documents.extend(document)\n",
    "        ids.append(file)\n",
    "    return documents, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e984bebf00f14123aa2b055d3392b54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documents, ids = create_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"google/flan-t5-small\"\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "db = Chroma.from_documents(documents, embeddings, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='N-Bit Precision (Basic)\\n\\nAudience: Users looking to train models faster and consume less memory.\\n\\nIf you’re looking to run models faster or consume less memory, consider tweaking the precision settings of your models.\\n\\nLower precision, such as 16-bit floating-point, requires less memory and enables training and deploying larger models.\\nHigher precision, such as the 64-bit floating-point, can be used for highly sensitive use-cases.\\n\\n16-bit Precision\\n\\nUse 16-bit mixed precision to lower your memory consumption by up to half so that you can train and deploy larger models. If your GPUs are [Tensor Core] GPUs, you can also get a ~3x speed improvement. Half precision can sometimes lead to unstable training.\\n\\nTrainer\\n\\nprecision\\n\\n\\'16-mixed\\'\\n\\n32-bit Precision\\n\\n32-bit precision is the default used across all models and research. This precision is known to be stable in contrast to lower precision settings.\\n\\nTrainer\\n\\nprecision\\n\\n\"32-true\"\\n\\n# or\\n\\nTrainer\\n\\nprecision\\n\\n\"32\"\\n\\n# or\\n\\nTrainer\\n\\nprecision\\n\\n32\\n\\n64-bit Precision\\n\\nFor certain scientific computations, 64-bit precision enables more accurate models. However, doubling the precision from 32 to 64 bit also doubles the memory requirements.\\n\\nTrainer\\n\\nprecision\\n\\n\"64-true\"\\n\\n# or\\n\\nTrainer\\n\\nprecision\\n\\n\"64\"\\n\\n# or\\n\\nTrainer\\n\\nprecision\\n\\n64\\n\\nNote\\n\\nSince in deep learning, memory is always a bottleneck, especially when dealing with a large volume of data and with limited resources.\\nIt is recommended using single precision for better speed. Although you can still use it if you want for your particular use-case.\\n\\nPrecision support by accelerator\\n\\nPrecision with Accelerators\\n\\nPrecision\\n\\nCPU\\n\\nGPU\\n\\nTPU\\n\\nIPU\\n\\n16 Mixed\\n\\nNo\\n\\nYes\\n\\nNo\\n\\nYes\\n\\nBFloat16 Mixed\\n\\nYes\\n\\nYes\\n\\nYes\\n\\nNo\\n\\n32 True\\n\\nYes\\n\\nYes\\n\\nYes\\n\\nYes\\n\\n64 True\\n\\nYes\\n\\nYes\\n\\nNo\\n\\nNo', metadata={'source': 'pl-docs/N-Bit Precision (Basic) — PyTorch Lightning 2.1.0dev documentation.html'}),\n",
       " Document(page_content='Installation\\n\\nApple Silicon (M1/M2/M3) Macs\\n\\nWhile ML related python packages are updated to work with Apple Silicon, you’ll need to set 2 environment variables on install.\\n\\n# needed for M1/M2/M3\\n\\nexport\\n\\nGRPC_PYTHON_BUILD_SYSTEM_OPENSSL\\n\\nexport\\n\\nGRPC_PYTHON_BUILD_SYSTEM_ZLIB\\n\\n1\\n\\npython\\n\\nm\\n\\npip\\n\\ninstall\\n\\nU\\n\\nlightning\\n\\nInstall with pip\\n\\nInstall lightning inside a virtual env or conda environment with pip\\n\\npython\\n\\nm\\n\\npip\\n\\ninstall\\n\\nlightning\\n\\nInstall with Conda\\n\\nIf you don’t have conda installed, follow the Conda Installation Guide.\\nLightning can be installed with conda using the following command:\\n\\nconda\\n\\ninstall\\n\\nlightning\\n\\nc\\n\\nconda-forge\\n\\nYou can also use Conda Environments:\\n\\nconda\\n\\nactivate\\n\\nmy_env\\nconda\\n\\ninstall\\n\\nlightning\\n\\nc\\n\\nconda-forge\\n\\nBuild from Source\\n\\nInstall nightly from the source. Note that it contains all the bug fixes and newly released features that\\nare not published yet. This is the bleeding edge, so use it at your own discretion.\\n\\npip\\n\\ninstall\\n\\nhttps://github.com/Lightning-AI/lightning/archive/refs/heads/master.zip\\n\\nU\\n\\nInstall future patch releases from the source. Note that the patch release contains only the bug fixes for the recent major release.\\n\\npip\\n\\ninstall\\n\\nhttps://github.com/Lightning-AI/lightning/archive/refs/heads/release/stable.zip\\n\\nU\\n\\nOptimized for model development\\n\\nIf you are deploying models built with Lightning in production and require few dependencies, try using the optimized lightning[pytorch] package:\\n\\npip\\n\\ninstall\\n\\nlightning\\n\\nCustom PyTorch Version\\n\\nTo use any PyTorch version visit the PyTorch Installation Page.\\n\\nOptimized for ML workflows (lightning Apps)\\n\\nIf you are deploying workflows built with Lightning in production and require fewer dependencies, try using the optimized lightning[apps] package:\\n\\npip\\n\\ninstall\\n\\nlightning-app', metadata={'source': 'pl-docs/Installation — PyTorch Lightning 2.1.0dev documentation.html'}),\n",
       " Document(page_content='Effective Training Techniques\\n\\nLightning implements various techniques to help during training that can help make the training smoother.\\n\\nAccumulate Gradients\\n\\nAccumulated gradients run K small batches of size N before doing a backward pass. The effect is a large effective batch size of size KxN, where N is the batch size.\\nInternally it doesn’t stack up the batches and do a forward pass rather it accumulates the gradients for K batches and then do an optimizer.step to make sure the\\neffective batch size is increased but there is no memory overhead.\\n\\nWarning\\n\\nloss.backward()\\n\\noptimizer.step()\\n\\nN*K\\n\\noptimizer.step()\\n\\nP*N*K\\n\\nN*K\\n\\n# DEFAULT (ie: no accumulated grads)\\n\\ntrainer\\n\\nTrainer\\n\\naccumulate_grad_batches\\n\\n# Accumulate gradients for 7 batches\\n\\ntrainer\\n\\nTrainer\\n\\naccumulate_grad_batches\\n\\nOptionally, you can make the accumulate_grad_batches value change over time by using the GradientAccumulationScheduler.\\nPass in a scheduling dictionary, where the key represents the epoch at which the value for gradient accumulation should be updated.\\n\\nfrom\\n\\nlightning.pytorch.callbacks\\n\\nimport\\n\\nGradientAccumulationScheduler\\n\\n# till 5th epoch, it will accumulate every 8 batches. From 5th epoch\\n\\n# till 9th epoch it will accumulate every 4 batches and after that no accumulation\\n\\n# will happen. Note that you need to use zero-indexed epoch keys here\\n\\naccumulator\\n\\nGradientAccumulationScheduler\\n\\nscheduling\\n\\n})\\n\\ntrainer\\n\\nTrainer\\n\\ncallbacks\\n\\naccumulator\\n\\nNote: Not all strategies and accelerators support variable gradient accumulation windows.\\n\\nGradient Clipping\\n\\nGradient clipping can be enabled to avoid exploding gradients. By default, this will clip the gradient norm by calling\\ntorch.nn.utils.clip_grad_norm_() computed over all model parameters together.\\nIf the Trainer’s gradient_clip_algorithm is set to \\'value\\' (\\'norm\\' by default), this will use instead\\ntorch.nn.utils.clip_grad_value_() for each parameter instead.\\n\\nNote\\n\\nIf using mixed precision, the gradient_clip_val does not need to be changed as the gradients are unscaled\\nbefore applying the clipping function.\\n\\nSee also\\n\\nTrainer\\n\\n# DEFAULT (ie: don\\'t clip)\\n\\ntrainer\\n\\nTrainer\\n\\ngradient_clip_val\\n\\n# clip gradients\\' global norm to <=0.5 using gradient_clip_algorithm=\\'norm\\' by default\\n\\ntrainer\\n\\nTrainer\\n\\ngradient_clip_val\\n\\n0.5\\n\\n# clip gradients\\' maximum magnitude to <=0.5\\n\\ntrainer\\n\\nTrainer\\n\\ngradient_clip_val\\n\\n0.5\\n\\ngradient_clip_algorithm\\n\\n\"value\"\\n\\nRead more about Configuring Gradient Clipping for advanced use-cases.\\n\\nStochastic Weight Averaging\\n\\nStochastic Weight Averaging (SWA) can make your models generalize better at virtually no additional cost.\\nThis can be used with both non-trained and trained models. The SWA procedure smooths the loss landscape thus making\\nit harder to end up in a local minimum during optimization.\\n\\nFor a more detailed explanation of SWA and how it works,\\nread this post by the PyTorch team.\\n\\nSee also\\n\\nThe StochasticWeightAveraging callback\\n\\n# Enable Stochastic Weight Averaging using the callback\\n\\ntrainer\\n\\nTrainer\\n\\ncallbacks\\n\\nStochasticWeightAveraging\\n\\nswa_lrs\\n\\n1e-2\\n\\n)])\\n\\nBatch Size Finder\\n\\nAuto-scaling of batch size can be enabled to find the largest batch size that fits into\\nmemory. Large batch size often yields a better estimation of the gradients, but may also result in\\nlonger training time. Inspired by https://github.com/BlackHC/toma.\\n\\nSee also\\n\\nTuner\\n\\nfrom\\n\\nlightning.pytorch.tuner\\n\\nimport\\n\\nTuner\\n\\n# Create a tuner for the trainer\\n\\ntrainer\\n\\nTrainer\\n\\n...\\n\\ntuner\\n\\nTuner\\n\\ntrainer\\n\\n# Auto-scale batch size by growing it exponentially (default)\\n\\ntuner\\n\\nscale_batch_size\\n\\nmodel\\n\\nmode\\n\\n\"power\"\\n\\n# Auto-scale batch size with binary search\\n\\ntuner\\n\\nscale_batch_size\\n\\nmodel\\n\\nmode\\n\\n\"binsearch\"\\n\\n# Fit as normal with new batch size\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nCurrently, this feature supports two modes \\'power\\' scaling and \\'binsearch\\'\\nscaling. In \\'power\\' scaling, starting from a batch size of 1 keeps doubling\\nthe batch size until an out-of-memory (OOM) error is encountered. Setting the\\nargument to \\'binsearch\\' will initially also try doubling the batch size until\\nit encounters an OOM, after which it will do a binary search that will finetune the\\nbatch size. Additionally, it should be noted that the batch size scaler cannot\\nsearch for batch sizes larger than the size of the training dataset.\\n\\nNote\\n\\nThis feature expects that a batch_size field is either located as a model attribute\\ni.e. model.batch_size or as a field in your hparams i.e. model.hparams.batch_size.\\nSimilarly it can work with datamodules too. The field should exist and will be updated by\\nthe results of this algorithm. Additionally, your train_dataloader() method should depend\\non this field for this feature to work i.e.\\n\\n# using LightningModule\\n\\nclass\\n\\nLitModel\\n\\nLightningModule\\n\\n):\\n\\ndef\\n\\n__init__\\n\\nself\\n\\nbatch_size\\n\\n):\\n\\nsuper\\n\\n()\\n\\n__init__\\n\\n()\\n\\nself\\n\\nsave_hyperparameters\\n\\n()\\n\\n# or\\n\\nself\\n\\nbatch_size\\n\\nbatch_size\\n\\ndef\\n\\ntrain_dataloader\\n\\nself\\n\\n):\\n\\nreturn\\n\\nDataLoader\\n\\ntrain_dataset\\n\\nbatch_size\\n\\nself\\n\\nbatch_size\\n\\nself\\n\\nhparams\\n\\nbatch_size\\n\\nmodel\\n\\nLitModel\\n\\nbatch_size\\n\\n32\\n\\ntrainer\\n\\nTrainer\\n\\n...\\n\\ntuner\\n\\nTuner\\n\\ntrainer\\n\\ntuner\\n\\nscale_batch_size\\n\\nmodel\\n\\n# using LightningDataModule\\n\\nclass\\n\\nLitDataModule\\n\\nLightningDataModule\\n\\n):\\n\\ndef\\n\\n__init__\\n\\nself\\n\\nbatch_size\\n\\n):\\n\\nsuper\\n\\n()\\n\\n__init__\\n\\n()\\n\\nself\\n\\nsave_hyperparameters\\n\\n()\\n\\n# or\\n\\nself\\n\\nbatch_size\\n\\nbatch_size\\n\\ndef\\n\\ntrain_dataloader\\n\\nself\\n\\n):\\n\\nreturn\\n\\nDataLoader\\n\\ntrain_dataset\\n\\nbatch_size\\n\\nself\\n\\nbatch_size\\n\\nself\\n\\nhparams\\n\\nbatch_size\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ndatamodule\\n\\nLitDataModule\\n\\nbatch_size\\n\\n32\\n\\ntrainer\\n\\nTrainer\\n\\n...\\n\\ntuner\\n\\nTuner\\n\\ntrainer\\n\\ntuner\\n\\nscale_batch_size\\n\\nmodel\\n\\ndatamodule\\n\\ndatamodule\\n\\ntrain_dataloader\\n\\nLightningModule\\n\\nLightningDataModule\\n\\nLightningModule\\n\\nLightningDataModule\\n\\ntrain_dataloader\\n\\nLightningDataModule\\n\\nDumping the current state of the model and trainer\\n\\nIteratively until convergence or maximum number of tries max_trials (default 25) has been reached:\\nCall fit() method of trainer. This evaluates steps_per_trial (default 3) number of\\noptimization steps. Each training step can trigger an OOM error if the tensors\\n(training batch, weights, gradients, etc.) allocated during the steps have a\\ntoo large memory footprint.\\nIf an OOM error is encountered, decrease batch size else increase it.\\nHow much the batch size is increased/decreased is determined by the chosen\\nstrategy.\\n\\n\\n\\n\\nThe found batch size is saved to either model.batch_size or model.hparams.batch_size\\nRestore the initial state of model and trainer\\n\\nWarning\\n\\nBatch size finder is not yet supported for DDP or any of its variations, it is coming soon.\\n\\nCustomizing Batch Size Finder\\n\\nWarning\\n\\nThis is an experimental feature.\\n\\nYou can also customize the BatchSizeFinder callback to run\\nat different epochs. This feature is useful while fine-tuning models since you can’t always use the same batch size after\\nunfreezing the backbone.\\n\\nfrom\\n\\nlightning.pytorch.callbacks\\n\\nimport\\n\\nBatchSizeFinder\\n\\nclass\\n\\nFineTuneBatchSizeFinder\\n\\nBatchSizeFinder\\n\\n):\\n\\ndef\\n\\n__init__\\n\\nself\\n\\nmilestones\\n\\nargs\\n\\n*\\n\\nkwargs\\n\\n):\\n\\nsuper\\n\\n()\\n\\n__init__\\n\\nargs\\n\\n*\\n\\nkwargs\\n\\nself\\n\\nmilestones\\n\\nmilestones\\n\\ndef\\n\\non_fit_start\\n\\nself\\n\\nargs\\n\\n*\\n\\nkwargs\\n\\n):\\n\\nreturn\\n\\ndef\\n\\non_train_epoch_start\\n\\nself\\n\\ntrainer\\n\\npl_module\\n\\n):\\n\\nif\\n\\ntrainer\\n\\ncurrent_epoch\\n\\nin\\n\\nself\\n\\nmilestones\\n\\nor\\n\\ntrainer\\n\\ncurrent_epoch\\n\\n==\\n\\nself\\n\\nscale_batch_size\\n\\ntrainer\\n\\npl_module\\n\\ntrainer\\n\\nTrainer\\n\\ncallbacks\\n\\nFineTuneBatchSizeFinder\\n\\nmilestones\\n\\n10\\n\\n))])\\n\\ntrainer\\n\\nfit\\n\\n...\\n\\nRun batch size finder for validate/test/predict.\\n\\nfrom\\n\\nlightning.pytorch.callbacks\\n\\nimport\\n\\nBatchSizeFinder\\n\\nclass\\n\\nEvalBatchSizeFinder\\n\\nBatchSizeFinder\\n\\n):\\n\\ndef\\n\\n__init__\\n\\nself\\n\\nargs\\n\\n*\\n\\nkwargs\\n\\n):\\n\\nsuper\\n\\n()\\n\\n__init__\\n\\nargs\\n\\n*\\n\\nkwargs\\n\\ndef\\n\\non_fit_start\\n\\nself\\n\\nargs\\n\\n*\\n\\nkwargs\\n\\n):\\n\\nreturn\\n\\ndef\\n\\non_test_start\\n\\nself\\n\\ntrainer\\n\\npl_module\\n\\n):\\n\\nself\\n\\nscale_batch_size\\n\\ntrainer\\n\\npl_module\\n\\ntrainer\\n\\nTrainer\\n\\ncallbacks\\n\\nEvalBatchSizeFinder\\n\\n()])\\n\\ntrainer\\n\\ntest\\n\\n...\\n\\nLearning Rate Finder\\n\\nFor training deep neural networks, selecting a good learning rate is essential\\nfor both better performance and faster convergence. Even optimizers such as\\nAdam that are self-adjusting the learning rate can benefit from more optimal\\nchoices.\\n\\nTo reduce the amount of guesswork concerning choosing a good initial learning\\nrate, a learning rate finder can be used. As described in this paper\\na learning rate finder does a small run where the learning rate is increased\\nafter each processed batch and the corresponding loss is logged. The result of\\nthis is a lr vs. loss plot that can be used as guidance for choosing an optimal\\ninitial learning rate.\\n\\nWarning\\n\\nFor the moment, this feature only works with models having a single optimizer.\\n\\nNote\\n\\nWith DDP: Since all the processes run in isolation, only process with global_rank=0 will make the decision to stop the\\nlearning rate finder and broadcast its results to all other ranks. That means, at the end of LR finder, each process will be running with\\nthe learning rate found on global_rank=0.\\n\\nUsing Lightning’s built-in LR finder\\n\\nlightning module needs to\\nhave a\\n\\nlearning_rate\\n\\nlr\\n\\nhparams\\n\\nhparams.learning_rate\\n\\nhparams.lr\\n\\nTuner via\\n\\ntuner\\n\\nTuner(trainer)\\n\\ntuner.lr_find(model)\\n\\nlearning_rate\\n\\nlightning module, which can be accessed\\nvia\\n\\nself.learning_rate\\n\\nself.lr\\n\\nfrom\\n\\nlightning.pytorch.tuner\\n\\nimport\\n\\nTuner\\n\\nclass\\n\\nLitModel\\n\\nLightningModule\\n\\n):\\n\\ndef\\n\\n__init__\\n\\nself\\n\\nlearning_rate\\n\\n):\\n\\nsuper\\n\\n()\\n\\n__init__\\n\\n()\\n\\nself\\n\\nlearning_rate\\n\\nlearning_rate\\n\\nself\\n\\nmodel\\n\\nModel\\n\\n...\\n\\ndef\\n\\nconfigure_optimizers\\n\\nself\\n\\n):\\n\\nreturn\\n\\nAdam\\n\\nself\\n\\nparameters\\n\\n(),\\n\\nlr\\n\\nself\\n\\nlr\\n\\nor\\n\\nself\\n\\nlearning_rate\\n\\n))\\n\\nmodel\\n\\nLitModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\n...\\n\\n# Create a Tuner\\n\\ntuner\\n\\nTuner\\n\\ntrainer\\n\\n# finds learning rate automatically\\n\\n# sets hparams.lr or hparams.learning_rate to that learning rate\\n\\ntuner\\n\\nlr_find\\n\\nmodel\\n\\nIf your model is using an arbitrary value instead of self.lr or self.learning_rate, set that value in lr_find:\\n\\nmodel\\n\\nLitModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\n...\\n\\ntuner\\n\\nTuner\\n\\ntrainer\\n\\n# to set to your own hparams.my_value\\n\\ntuner\\n\\nlr_find\\n\\nmodel\\n\\nattr_name\\n\\n\"my_value\"\\n\\nYou can also inspect the results of the learning rate finder or just play around\\nwith the parameters of the algorithm. A typical example of this would look like:\\n\\nmodel\\n\\nMyModelClass\\n\\nhparams\\n\\ntrainer\\n\\nTrainer\\n\\n()\\n\\ntuner\\n\\nTuner\\n\\ntrainer\\n\\n# Run learning rate finder\\n\\nlr_finder\\n\\ntuner\\n\\nlr_find\\n\\nmodel\\n\\n# Results can be found in\\n\\nprint\\n\\nlr_finder\\n\\nresults\\n\\n# Plot with\\n\\nfig\\n\\nlr_finder\\n\\nplot\\n\\nsuggest\\n\\nTrue\\n\\nfig\\n\\nshow\\n\\n()\\n\\n# Pick point based on plot, or get suggestion\\n\\nnew_lr\\n\\nlr_finder\\n\\nsuggestion\\n\\n()\\n\\n# update hparams of the model\\n\\nmodel\\n\\nhparams\\n\\nlr\\n\\nnew_lr\\n\\n# Fit model\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nThe figure produced by lr_finder.plot() should look something like the figure\\nbelow. It is recommended to not pick the learning rate that achieves the lowest\\nloss, but instead something in the middle of the sharpest downward slope (red point).\\nThis is the point returned py lr_finder.suggestion().\\n\\nCustomizing Learning Rate Finder\\n\\nWarning\\n\\nThis is an experimental feature.\\n\\nYou can also customize the LearningRateFinder callback to run at different epochs. This feature is useful while fine-tuning models.\\n\\nfrom\\n\\nlightning.pytorch.callbacks\\n\\nimport\\n\\nLearningRateFinder\\n\\nclass\\n\\nFineTuneLearningRateFinder\\n\\nLearningRateFinder\\n\\n):\\n\\ndef\\n\\n__init__\\n\\nself\\n\\nmilestones\\n\\nargs\\n\\n*\\n\\nkwargs\\n\\n):\\n\\nsuper\\n\\n()\\n\\n__init__\\n\\nargs\\n\\n*\\n\\nkwargs\\n\\nself\\n\\nmilestones\\n\\nmilestones\\n\\ndef\\n\\non_fit_start\\n\\nself\\n\\nargs\\n\\n*\\n\\nkwargs\\n\\n):\\n\\nreturn\\n\\ndef\\n\\non_train_epoch_start\\n\\nself\\n\\ntrainer\\n\\npl_module\\n\\n):\\n\\nif\\n\\ntrainer\\n\\ncurrent_epoch\\n\\nin\\n\\nself\\n\\nmilestones\\n\\nor\\n\\ntrainer\\n\\ncurrent_epoch\\n\\n==\\n\\nself\\n\\nlr_find\\n\\ntrainer\\n\\npl_module\\n\\ntrainer\\n\\nTrainer\\n\\ncallbacks\\n\\nFineTuneLearningRateFinder\\n\\nmilestones\\n\\n10\\n\\n))])\\n\\ntrainer\\n\\nfit\\n\\n...\\n\\nAdvanced GPU Optimizations\\n\\nWhen training on single or multiple GPU machines, Lightning offers a host of advanced optimizations to improve throughput, memory efficiency, and model scaling.\\nRefer to Advanced GPU Optimized Training for more details.\\n\\nSharing Datasets Across Process Boundaries\\n\\nThe LightningDataModule class provides an organized way to decouple data loading from training logic, with prepare_data() being used for downloading and pre-processing the dataset on a single process, and setup() loading the pre-processed data for each process individually:\\n\\nclass\\n\\nMNISTDataModule\\n\\npl\\n\\nLightningDataModule\\n\\n):\\n\\ndef\\n\\nprepare_data\\n\\nself\\n\\n):\\n\\nMNIST\\n\\nself\\n\\ndata_dir\\n\\ndownload\\n\\nTrue\\n\\ndef\\n\\nsetup\\n\\nself\\n\\nstage\\n\\nstr\\n\\n):\\n\\nself\\n\\nmnist\\n\\nMNIST\\n\\nself\\n\\ndata_dir\\n\\ndef\\n\\ntrain_loader\\n\\nself\\n\\n):\\n\\nreturn\\n\\nDataLoader\\n\\nself\\n\\nmnist\\n\\nbatch_size\\n\\n128\\n\\nHowever, for in-memory datasets, that means that each process will hold a (redundant) replica of the dataset in memory, which may be impractical when using many processes while utilizing datasets that nearly fit into CPU memory, as the memory consumption will scale up linearly with the number of processes.\\nFor example, when training Graph Neural Networks, a common strategy is to load the entire graph into CPU memory for fast access to the entire graph structure and its features, and to then perform neighbor sampling to obtain mini-batches that fit onto the GPU.\\n\\nA simple way to prevent redundant dataset replicas is to rely on torch.multiprocessing to share the data automatically between spawned processes via shared memory.\\nFor this, all data pre-loading should be done on the main process inside DataModule.__init__(). As a result, all tensor-data will get automatically shared when using the \\'ddp_spawn\\' strategy.\\n\\nWarning\\n\\ntorch.multiprocessing will send a handle of each individual tensor to other processes.\\nIn order to prevent any errors due to too many open file handles, try to reduce the number of tensors to share, e.g., by stacking your data into a single tensor.\\n\\nclass\\n\\nMNISTDataModule\\n\\npl\\n\\nLightningDataModule\\n\\n):\\n\\ndef\\n\\n__init__\\n\\nself\\n\\ndata_dir\\n\\nstr\\n\\n):\\n\\nself\\n\\nmnist\\n\\nMNIST\\n\\ndata_dir\\n\\ndownload\\n\\nTrue\\n\\ntransform\\n\\nToTensor\\n\\n())\\n\\ndef\\n\\ntrain_loader\\n\\nself\\n\\n):\\n\\nreturn\\n\\nDataLoader\\n\\nself\\n\\nmnist\\n\\nbatch_size\\n\\n128\\n\\nmodel\\n\\nModel\\n\\n...\\n\\ndatamodule\\n\\nMNISTDataModule\\n\\n\"data/MNIST\"\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"ddp_spawn\"\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\ndatamodule\\n\\nSee the graph-level and node-level prediction examples in PyTorch Geometric for practical use-cases.', metadata={'source': 'pl-docs/Effective Training Techniques — PyTorch Lightning 2.1.0dev documentation.html'}),\n",
       " Document(page_content='Train 1 trillion+ parameter models\\n\\nWhen training large models, fitting larger batch sizes, or trying to increase throughput using multi-GPU compute, Lightning provides advanced optimized distributed training strategies to support these cases and offer substantial improvements in memory usage.\\n\\nNote that some of the extreme memory saving configurations will affect the speed of training. This Speed/Memory trade-off in most cases can be adjusted.\\n\\nSome of these memory-efficient strategies rely on offloading onto other forms of memory, such as CPU RAM or NVMe. This means you can even see memory benefits on a single GPU, using a strategy such as DeepSpeed ZeRO Stage 3 Offload.\\n\\nCheck out this amazing video explaining model parallelism and how it works behind the scenes:\\n\\nChoosing an Advanced Distributed GPU Strategy\\n\\nIf you would like to stick with PyTorch DDP, see DDP Optimizations.\\n\\nUnlike DistributedDataParallel (DDP) where the maximum trainable model size and batch size do not change with respect to the number of GPUs, memory-optimized strategies can accommodate bigger models and larger batches as more GPUs are used. This means as you scale up the number of GPUs, you can reach the number of model parameters you’d like to train.\\n\\nThere are many considerations when choosing a strategy as described below. In addition, check out the visualization of various strategy benchmarks using minGPT here.\\n\\nPre-training vs Fine-tuning\\n\\nWhen fine-tuning, we often use a magnitude less data compared to pre-training a model. This is important when choosing a distributed strategy as usually for pre-training, we are compute-bound.\\nThis means we cannot sacrifice throughput as much as if we were fine-tuning, because in fine-tuning the data requirement is smaller.\\n\\nOverall:\\n\\nWhen fine-tuning a model, use advanced memory efficient strategies such as Fully Sharded Training, DeepSpeed ZeRO Stage 3 or DeepSpeed ZeRO Stage 3 Offload, allowing you to fine-tune larger models if you are limited on compute\\n\\nWhen pre-training a model, use simpler optimizations such as DeepSpeed ZeRO Stage 2, scaling the number of GPUs to reach larger parameter sizes\\n\\nFor both fine-tuning and pre-training, use DeepSpeed Activation Checkpointing as the throughput degradation is not significant\\n\\nFor example when using 128 GPUs, you can pre-train large 10 to 20 Billion parameter models using DeepSpeed ZeRO Stage 2 without having to take a performance hit with more advanced optimized multi-gpu strategy.\\n\\nBut for fine-tuning a model, you can reach 10 to 20 Billion parameter models using DeepSpeed ZeRO Stage 3 Offload on a single GPU. This does come with a significant throughput hit, which needs to be weighed accordingly.\\n\\nWhen Shouldn’t I use an Optimized Distributed Strategy?\\n\\nSharding techniques help when model sizes are fairly large; roughly 500M+ parameters is where we’ve seen benefits. However, in the following cases, we recommend sticking to ordinary distributed strategies\\n\\nWhen your model is small (ResNet50 of around 80M Parameters), unless you are using unusually large batch sizes or inputs.\\n\\nDue to high distributed communication between devices, if running on a slow network/interconnect, the training might be much slower than expected and then it’s up to you to determince the tradeoff here.\\n\\nCutting-edge and third-party Strategies\\n\\nCutting-edge Lightning strategies are being developed by third-parties outside of Lightning.\\n\\nIf you want to try some of the latest and greatest features for model-parallel training, check out the Colossal-AI Strategy integration.\\n\\nAnother integration is Bagua Strategy, deep learning training acceleration framework for PyTorch, with advanced distributed training algorithms and system optimizations.\\n\\nFor training on unreliable mixed GPUs across the internet check out the Hivemind Strategy integration.\\n\\nFully Sharded Training\\n\\nPyTorch has it’s own version of FSDP which is upstreamed from their fairscale project.\\nIt was introduced in their v1.11.0 release but it is recommended to use it with PyTorch v1.12 or more and that’s what\\nLightning supports.\\n\\nWarning\\n\\nThis is an experimental feature.\\n\\nAuto Wrapping\\n\\nModel layers should be wrapped in FSDP in a nested way to save peak memory and enable communication and computation overlapping. The\\nsimplest way to do it is auto wrapping, which can serve as a drop-in replacement for DDP without changing the rest of the code. You don’t\\nhave to wrap layers manually as in the case of manual wrapping.\\n\\nNote\\n\\nWhile initializing the optimizers inside configure_optimizers hook, make sure to use self.trainer.model.parameters(), else\\nPyTorch will raise an error. This is required because when you use auto-wrap, the model layers are sharded and your\\nlightning_module.parameters() will return a generator with no params. This inconvenience will be addressed in the future.\\n\\nmodel\\n\\nBoringModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"fsdp\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nRead more here.\\n\\nManual Wrapping\\n\\nManual wrapping can be useful to explore complex sharding strategies by applying wrap selectively to some parts of the model. To activate\\nparameter sharding with manual wrapping, you can wrap your model using the wrap function. Internally in Lightning, we enable a context manager around the configure_sharded_model function to make sure the wrap parameters are passed correctly.\\n\\nWhen not using Fully Sharded these wrap functions are a no-op. This means once the changes have been made, there is no need to remove the changes for other strategies.\\n\\nwrap simply wraps the module with a Fully Sharded Parallel class with the correct parameters from the Lightning context manager.\\n\\nHere’s an example using that uses wrap to create your model:\\n\\nimport\\n\\ntorch\\n\\nimport\\n\\ntorch.nn\\n\\nas\\n\\nnn\\n\\nimport\\n\\nlightning.pytorch\\n\\nas\\n\\npl\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\ntorch.distributed.fsdp.wrap\\n\\nimport\\n\\nwrap\\n\\nclass\\n\\nMyModel\\n\\npl\\n\\nLightningModule\\n\\n):\\n\\ndef\\n\\n__init__\\n\\nself\\n\\n):\\n\\nsuper\\n\\n()\\n\\n__init__\\n\\n()\\n\\nself\\n\\nlinear_layer\\n\\nnn\\n\\nLinear\\n\\n32\\n\\n32\\n\\nself\\n\\nblock\\n\\nnn\\n\\nSequential\\n\\nnn\\n\\nLinear\\n\\n32\\n\\n32\\n\\n),\\n\\nnn\\n\\nLinear\\n\\n32\\n\\n32\\n\\n))\\n\\ndef\\n\\nconfigure_sharded_model\\n\\nself\\n\\n):\\n\\n# modules are sharded across processes\\n\\n# as soon as they are wrapped with `wrap`.\\n\\n# During the forward/backward passes, weights get synced across processes\\n\\n# and de-allocated once computation is complete, saving memory.\\n\\n# Wraps the layer in a Fully Sharded Wrapper automatically\\n\\nlinear_layer\\n\\nwrap\\n\\nself\\n\\nlinear_layer\\n\\nfor\\n\\nlayer\\n\\nin\\n\\nenumerate\\n\\nself\\n\\nblock\\n\\n):\\n\\nself\\n\\nblock\\n\\nwrap\\n\\nlayer\\n\\nself\\n\\nmodel\\n\\nnn\\n\\nSequential\\n\\nlinear_layer\\n\\nnn\\n\\nReLU\\n\\n(),\\n\\nself\\n\\nblock\\n\\ndef\\n\\nconfigure_optimizers\\n\\nself\\n\\n):\\n\\nreturn\\n\\ntorch\\n\\noptim\\n\\nAdamW\\n\\nself\\n\\nmodel\\n\\nparameters\\n\\n())\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"fsdp\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nYou can customize the strategy configuration by adjusting the arguments of FSDPStrategy and pass that to the strategy argument inside the Trainer.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nFSDPStrategy\\n\\nfsdp\\n\\nFSDPStrategy\\n\\ncpu_offload\\n\\nTrue\\n\\n# equivalent to passing `\"fsdp_cpu_offload\"`\\n\\ntrainer\\n\\npl\\n\\nTrainer\\n\\nstrategy\\n\\nfsdp\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nCheck out this tutorial to learn more about it.\\n\\nActivation Checkpointing\\n\\nActivation checkpointing reduces GPU memory usage by avoiding the storage of intermediate activation tensors in\\nselected layers. The tradeoff is that computation cost for the backpropagation increases, as the dropped activations\\nneed to be recomputed.\\n\\nEnable checkpointing on large layers (like Transformers) by providing the layer class/type to the strategy:\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nFSDPStrategy\\n\\nfsdp\\n\\nFSDPStrategy\\n\\nactivation_checkpointing\\n\\nMyTransformerBlock\\n\\n# or pass a list with multiple types\\n\\ntrainer\\n\\npl\\n\\nTrainer\\n\\nstrategy\\n\\nfsdp\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nDeepSpeed\\n\\nDeepSpeed is a deep learning training optimization library, providing the means to train massive billion parameter models at scale.\\nUsing the DeepSpeed strategy, we were able to train model sizes of 10 Billion parameters and above, with a lot of useful information in this benchmark and the DeepSpeed docs.\\nDeepSpeed also offers lower level training optimizations, and efficient optimizers such as 1-bit Adam. We recommend using DeepSpeed in environments where speed and memory optimizations are important (such as training large billion parameter models).\\n\\nWarning\\n\\nThis is an experimental feature.\\n\\nBelow is a summary of all the configurations of DeepSpeed.\\n\\nDeepSpeed ZeRO Stage 1 - Shard optimizer states, remains at speed parity with DDP whilst providing memory improvement\\n\\nDeepSpeed ZeRO Stage 2 - Shard optimizer states and gradients, remains at speed parity with DDP whilst providing even more memory improvement\\n\\nDeepSpeed ZeRO Stage 2 Offload - Offload optimizer states and gradients to CPU. Increases distributed communication volume and GPU-CPU device transfer, but provides significant memory improvement\\n\\nDeepSpeed ZeRO Stage 3 - Shard optimizer states, gradients, parameters and optionally activations. Increases distributed communication volume, but provides even more memory improvement\\n\\nDeepSpeed ZeRO Stage 3 Offload - Offload optimizer states, gradients, parameters and optionally activations to CPU. Increases distributed communication volume and GPU-CPU device transfer, but even more significant memory improvement.\\n\\nDeepSpeed Activation Checkpointing - Free activations after forward pass. Increases computation, but provides memory improvement for all stages.\\n\\nTo use DeepSpeed, you first need to install DeepSpeed using the commands below.\\n\\npip\\n\\ninstall\\n\\ndeepspeed\\n\\nIf you run into an issue with the install or later in training, ensure that the CUDA version of the PyTorch you’ve installed matches your locally installed CUDA (you can see which one has been recognized by running nvcc --version).\\n\\nNote\\n\\nDeepSpeed currently only supports single optimizer, single scheduler within the training loop.\\n\\nWhen saving a checkpoint we rely on DeepSpeed which saves a directory containing the model and various components.\\n\\nDeepSpeed ZeRO Stage 1\\n\\nDeepSpeed ZeRO Stage 1 partitions your optimizer states (Stage 1) across your GPUs to reduce memory.\\n\\nIt is recommended to skip Stage 1 and use Stage 2, which comes with larger memory improvements and still remains efficient. Stage 1 is useful to pair with certain optimizations such as Torch ORT.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_1\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nDeepSpeed ZeRO Stage 2\\n\\nDeepSpeed ZeRO Stage 2 partitions your optimizer states (Stage 1) and your gradients (Stage 2) across your GPUs to reduce memory. In most cases, this is more efficient or at parity with DDP, primarily due to the optimized custom communications written by the DeepSpeed team.\\nAs a result, benefits can also be seen on a single GPU. Do note that the default bucket sizes allocate around 3.6GB of VRAM to use during distributed communications, which can be tweaked when instantiating the strategy described in a few sections below.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_2\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\npython\\n\\ntrain.py\\n\\n-strategy\\n\\ndeepspeed_stage_2\\n\\n-precision\\n\\n16\\n\\n-accelerator\\n\\n\\'gpu\\'\\n\\n-devices\\n\\nDeepSpeed ZeRO Stage 2 Offload\\n\\nBelow we show an example of running ZeRO-Offload. ZeRO-Offload leverages the host CPU to offload optimizer memory/computation, reducing the overall memory consumption.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_2_offload\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nThis can also be done via the command line using a PyTorch Lightning script:\\n\\npython\\n\\ntrain.py\\n\\n-strategy\\n\\ndeepspeed_stage_2_offload\\n\\n-precision\\n\\n16\\n\\n-accelerator\\n\\n\\'gpu\\'\\n\\n-devices\\n\\nYou can also modify the ZeRO-Offload parameters via the strategy as below.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDeepSpeedStrategy\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDeepSpeedStrategy\\n\\noffload_optimizer\\n\\nTrue\\n\\nallgather_bucket_size\\n\\n5e8\\n\\nreduce_bucket_size\\n\\n5e8\\n\\n),\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nNote\\n\\nWe suggest tuning the allgather_bucket_size parameter and reduce_bucket_size parameter to find optimum parameters based on your model size.\\nThese control how large a buffer we limit the model to using when reducing gradients/gathering updated parameters. Smaller values will result in less memory, but tradeoff with speed.\\n\\nDeepSpeed allocates a reduce buffer size multiplied by 1.5x so take that into consideration when tweaking the parameters.\\n\\nThe strategy sets a reasonable default of 2e8, which should work for most low VRAM GPUs (less than 7GB), allocating roughly 3.6GB of VRAM as buffer. Higher VRAM GPUs should aim for values around 5e8.\\n\\nFor even more speed benefit, DeepSpeed offers an optimized CPU version of ADAM called DeepSpeedCPUAdam to run the offloaded computation, which is faster than the standard PyTorch implementation.\\n\\nimport\\n\\nlightning.pytorch\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\ndeepspeed.ops.adam\\n\\nimport\\n\\nDeepSpeedCPUAdam\\n\\nclass\\n\\nMyModel\\n\\npl\\n\\nLightningModule\\n\\n):\\n\\n...\\n\\ndef\\n\\nconfigure_optimizers\\n\\nself\\n\\n):\\n\\n# DeepSpeedCPUAdam provides 5x to 7x speedup over torch.optim.adam(w)\\n\\nreturn\\n\\nDeepSpeedCPUAdam\\n\\nself\\n\\nparameters\\n\\n())\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_2_offload\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nDeepSpeed ZeRO Stage 3\\n\\nDeepSpeed ZeRO Stage 3 shards the optimizer states, gradients and the model parameters (also optionally activations). Sharding model parameters and activations comes with an increase in distributed communication, however allows you to scale your models massively from one GPU to multiple GPUs.\\nThe DeepSpeed team report the ability to fine-tune models with over 40B parameters on a single GPU and over 2 Trillion parameters on 512 GPUs. For more information we suggest checking the DeepSpeed ZeRO-3 Offload documentation.\\n\\nWe’ve ran benchmarks for all these features and given a simple example of how all these features work in Lightning, which you can see at minGPT.\\n\\nTo reach the highest memory efficiency or model size, you must:\\n\\nUse the DeepSpeed strategy with the stage 3 parameter\\n\\nUse CPU Offloading to offload weights to CPU, plus have a reasonable amount of CPU RAM to offload onto\\n\\nUse DeepSpeed Activation Checkpointing to shard activations\\n\\nBelow we describe how to enable all of these to see benefit. With all these improvements we reached 45 Billion parameters training a GPT model on 8 GPUs with ~1TB of CPU RAM available.\\n\\nAlso please have a look at our DeepSpeed ZeRO Stage 3 Tips which contains a lot of helpful information when configuring your own models.\\n\\nNote\\n\\nWhen saving a model using DeepSpeed and Stage 3, model states and optimizer states will be saved in separate sharded states (based on the world size). See Collating Single File Checkpoint for DeepSpeed ZeRO Stage 3 to obtain a single checkpoint file.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\ndeepspeed.ops.adam\\n\\nimport\\n\\nFusedAdam\\n\\nclass\\n\\nMyModel\\n\\npl\\n\\nLightningModule\\n\\n):\\n\\n...\\n\\ndef\\n\\nconfigure_optimizers\\n\\nself\\n\\n):\\n\\nreturn\\n\\nFusedAdam\\n\\nself\\n\\nparameters\\n\\n())\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_3\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\ntrainer\\n\\ntest\\n\\n()\\n\\ntrainer\\n\\npredict\\n\\n()\\n\\nYou can also use the Lightning Trainer to run predict or evaluate with DeepSpeed once the model has been trained.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nclass\\n\\nMyModel\\n\\npl\\n\\nLightningModule\\n\\n):\\n\\n...\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_3\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\ntest\\n\\nckpt_path\\n\\n\"my_saved_deepspeed_checkpoint.ckpt\"\\n\\nShard Model Instantly to Reduce Initialization Time/Memory\\n\\nWhen instantiating really large models, it is sometimes necessary to shard the model layers instantly.\\n\\nThis is the case if layers may not fit on one single machines CPU or GPU memory, but would fit once sharded across multiple machines.\\nWe expose a hook that layers initialized within the hook will be sharded instantly on a per layer basis, allowing you to instantly shard models.\\n\\nThis reduces the time taken to initialize very large models, as well as ensure we do not run out of memory when instantiating larger models. For more information you can refer to the DeepSpeed docs for Constructing Massive Models.\\n\\nimport\\n\\ntorch.nn\\n\\nas\\n\\nnn\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\ndeepspeed.ops.adam\\n\\nimport\\n\\nFusedAdam\\n\\nclass\\n\\nMyModel\\n\\npl\\n\\nLightningModule\\n\\n):\\n\\n...\\n\\ndef\\n\\nconfigure_sharded_model\\n\\nself\\n\\n):\\n\\n# Created within sharded model context, modules are instantly sharded across processes\\n\\n# as soon as they are made.\\n\\nself\\n\\nblock\\n\\nnn\\n\\nSequential\\n\\nnn\\n\\nLinear\\n\\n32\\n\\n32\\n\\n),\\n\\nnn\\n\\nReLU\\n\\n())\\n\\ndef\\n\\nconfigure_optimizers\\n\\nself\\n\\n):\\n\\nreturn\\n\\nFusedAdam\\n\\nself\\n\\nparameters\\n\\n())\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_3\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\ntrainer\\n\\ntest\\n\\n()\\n\\ntrainer\\n\\npredict\\n\\n()\\n\\nDeepSpeed ZeRO Stage 3 Offload\\n\\nDeepSpeed ZeRO Stage 3 Offloads optimizer state, gradients to the host CPU to reduce memory usage as ZeRO Stage 2 does, however additionally allows you to offload the parameters as well for even more memory saving.\\n\\nNote\\n\\nWhen saving a model using DeepSpeed and Stage 3, model states and optimizer states will be saved in separate sharded states (based on the world size). See Collating Single File Checkpoint for DeepSpeed ZeRO Stage 3 to obtain a single checkpoint file.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDeepSpeedStrategy\\n\\n# Enable CPU Offloading\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_3_offload\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\n# Enable CPU Offloading, and offload parameters to CPU\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDeepSpeedStrategy\\n\\nstage\\n\\noffload_optimizer\\n\\nTrue\\n\\noffload_parameters\\n\\nTrue\\n\\n),\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nDeepSpeed Infinity (NVMe Offloading)\\n\\nAdditionally, DeepSpeed supports offloading to NVMe drives for even larger models, utilizing the large memory space found in NVMes. DeepSpeed reports the ability to fine-tune 1 Trillion+ parameters using NVMe Offloading on one 8 GPU machine. Below shows how to enable this, assuming the NVMe drive is mounted in a directory called /local_nvme.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDeepSpeedStrategy\\n\\n# Enable CPU Offloading\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_3_offload\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\n# Enable CPU Offloading, and offload parameters to CPU\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDeepSpeedStrategy\\n\\nstage\\n\\noffload_optimizer\\n\\nTrue\\n\\noffload_parameters\\n\\nTrue\\n\\nremote_device\\n\\n\"nvme\"\\n\\noffload_params_device\\n\\n\"nvme\"\\n\\noffload_optimizer_device\\n\\n\"nvme\"\\n\\nnvme_path\\n\\n\"/local_nvme\"\\n\\n),\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nWhen offloading to NVMe you may notice that the speed is slow. There are parameters that need to be tuned based on the drives that you are using. Running the aio_bench_perf_sweep.py script can help you to find optimum parameters. See the issue for more information on how to parse the information.\\n\\nDeepSpeed Activation Checkpointing\\n\\nActivation checkpointing frees activations from memory as soon as they are not needed during the forward pass.\\nThey are then re-computed for the backwards pass as needed.\\n\\nActivation checkpointing is very useful when you have intermediate layers that produce large activations.\\n\\nThis saves memory when training larger models, however requires using a checkpoint function to run modules as shown below.\\n\\nWarning\\n\\nEnsure to not wrap the entire model with activation checkpointing. This is not the intended usage of activation checkpointing, and will lead to failures as seen in this discussion.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nimport\\n\\ndeepspeed\\n\\nclass\\n\\nMyModel\\n\\nLightningModule\\n\\n):\\n\\n...\\n\\ndef\\n\\n__init__\\n\\nself\\n\\n):\\n\\nsuper\\n\\n()\\n\\n__init__\\n\\n()\\n\\nself\\n\\nblock_1\\n\\nnn\\n\\nSequential\\n\\nnn\\n\\nLinear\\n\\n32\\n\\n32\\n\\n),\\n\\nnn\\n\\nReLU\\n\\n())\\n\\nself\\n\\nblock_2\\n\\ntorch\\n\\nnn\\n\\nLinear\\n\\n32\\n\\ndef\\n\\nforward\\n\\nself\\n\\n):\\n\\n# Use the DeepSpeed checkpointing function instead of calling the module directly\\n\\n# checkpointing self.block_1 means the activations are deleted after use,\\n\\n# and re-calculated during the backward passes\\n\\ndeepspeed\\n\\ncheckpointing\\n\\ncheckpoint\\n\\nself\\n\\nblock_1\\n\\nreturn\\n\\nself\\n\\nblock_2\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDeepSpeedStrategy\\n\\nimport\\n\\ndeepspeed\\n\\nclass\\n\\nMyModel\\n\\npl\\n\\nLightningModule\\n\\n):\\n\\n...\\n\\ndef\\n\\nconfigure_sharded_model\\n\\nself\\n\\n):\\n\\nself\\n\\nblock_1\\n\\nnn\\n\\nSequential\\n\\nnn\\n\\nLinear\\n\\n32\\n\\n32\\n\\n),\\n\\nnn\\n\\nReLU\\n\\n())\\n\\nself\\n\\nblock_2\\n\\ntorch\\n\\nnn\\n\\nLinear\\n\\n32\\n\\ndef\\n\\nforward\\n\\nself\\n\\n):\\n\\n# Use the DeepSpeed checkpointing function instead of calling the module directly\\n\\ndeepspeed\\n\\ncheckpointing\\n\\ncheckpoint\\n\\nself\\n\\nblock_1\\n\\nreturn\\n\\nself\\n\\nblock_2\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_3_offload\"\\n\\nprecision\\n\\n16\\n\\n# Enable CPU Activation Checkpointing\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDeepSpeedStrategy\\n\\nstage\\n\\noffload_optimizer\\n\\nTrue\\n\\n# Enable CPU Offloading\\n\\ncpu_checkpointing\\n\\nTrue\\n\\n# (Optional) offload activations to CPU\\n\\n),\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nDeepSpeed ZeRO Stage 3 Tips\\n\\nHere is some helpful information when setting up DeepSpeed ZeRO Stage 3 with Lightning.\\n\\nIf you’re using Adam or AdamW, ensure to use FusedAdam or DeepSpeedCPUAdam (for CPU Offloading) rather than the default torch optimizers as they come with large speed benefits\\n\\nTreat your GPU/CPU memory as one large pool. In some cases, you may not want to offload certain things (like activations) to provide even more space to offload model parameters\\n\\nWhen offloading to the CPU, make sure to bump up the batch size as GPU memory will be freed\\n\\nWe also support sharded checkpointing. By passing save_full_weights=False to the DeepSpeedStrategy, we’ll save shards of the model which allows you to save extremely large models. However to load the model and run test/validation/predict you must use the Trainer object.\\n\\nCollating Single File Checkpoint for DeepSpeed ZeRO Stage 3\\n\\nAfter training using ZeRO Stage 3, you’ll notice that your checkpoints are a directory of sharded model and optimizer states. If you’d like to collate a single file from the checkpoint directory please use the below command, which handles all the Lightning states additionally when collating the file.\\n\\nfrom\\n\\nlightning.pytorch.utilities.deepspeed\\n\\nimport\\n\\nconvert_zero_checkpoint_to_fp32_state_dict\\n\\n# lightning deepspeed has saved a directory instead of a file\\n\\nsave_path\\n\\n\"lightning_logs/version_0/checkpoints/epoch=0-step=0.ckpt/\"\\n\\noutput_path\\n\\n\"lightning_model.pt\"\\n\\nconvert_zero_checkpoint_to_fp32_state_dict\\n\\nsave_path\\n\\noutput_path\\n\\nWarning\\n\\nThis single file checkpoint does not include the optimizer/lr-scheduler states. This means we cannot restore training via the trainer.fit(ckpt_path=) call. Ensure to keep the sharded checkpoint directory if this is required.\\n\\nCustom DeepSpeed Config\\n\\nIn some cases you may want to define your own DeepSpeed Config, to access all parameters defined. We’ve exposed most of the important parameters, however, there may be debugging parameters to enable. Also, DeepSpeed allows the use of custom DeepSpeed optimizers and schedulers defined within a config file that is supported.\\n\\nNote\\n\\nAll strategy default parameters will be ignored when a config object is passed.\\nAll compatible arguments can be seen in the DeepSpeed docs.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDeepSpeedStrategy\\n\\ndeepspeed_config\\n\\n\"zero_allow_untested_optimizer\"\\n\\nTrue\\n\\n\"optimizer\"\\n\\n\"type\"\\n\\n\"OneBitAdam\"\\n\\n\"params\"\\n\\n\"lr\"\\n\\n3e-5\\n\\n\"betas\"\\n\\n0.998\\n\\n0.999\\n\\n],\\n\\n\"eps\"\\n\\n1e-5\\n\\n\"weight_decay\"\\n\\n1e-9\\n\\n\"cuda_aware\"\\n\\nTrue\\n\\n},\\n\\n},\\n\\n\"scheduler\"\\n\\n\"type\"\\n\\n\"WarmupLR\"\\n\\n\"params\"\\n\\n\"last_batch_iteration\"\\n\\n\"warmup_min_lr\"\\n\\n\"warmup_max_lr\"\\n\\n3e-5\\n\\n\"warmup_num_steps\"\\n\\n100\\n\\n},\\n\\n},\\n\\n\"zero_optimization\"\\n\\n\"stage\"\\n\\n# Enable Stage 2 ZeRO (Optimizer/Gradient state partitioning)\\n\\n\"offload_optimizer\"\\n\\nTrue\\n\\n# Enable Offloading optimizer state/calculation to the host CPU\\n\\n\"contiguous_gradients\"\\n\\nTrue\\n\\n# Reduce gradient fragmentation.\\n\\n\"overlap_comm\"\\n\\nTrue\\n\\n# Overlap reduce/backward operation of gradients for speed.\\n\\n\"allgather_bucket_size\"\\n\\n2e8\\n\\n# Number of elements to all gather at once.\\n\\n\"reduce_bucket_size\"\\n\\n2e8\\n\\n# Number of elements we reduce/allreduce at once.\\n\\n},\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDeepSpeedStrategy\\n\\nconfig\\n\\ndeepspeed_config\\n\\n),\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nWe support taking the config as a json formatted file:\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDeepSpeedStrategy\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDeepSpeedStrategy\\n\\nconfig\\n\\n\"/path/to/deepspeed_config.json\"\\n\\n),\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nYou can use also use an environment variable via your PyTorch Lightning script:\\n\\nPL_DEEPSPEED_CONFIG_PATH\\n\\n=/path/to/deepspeed_config.json\\n\\npython\\n\\ntrain.py\\n\\n-strategy\\n\\ndeepspeed\\n\\nDDP Optimizations\\n\\nDDP Static Graph\\n\\nDDP static graph assumes that your model\\nemploys the same set of used/unused parameters in every iteration, so that it can deterministically know the flow of\\ntraining and apply special optimizations during runtime.\\n\\nNote\\n\\nDDP static graph support requires PyTorch>=1.11.0\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDDPStrategy\\n\\ntrainer\\n\\nTrainer\\n\\ndevices\\n\\nstrategy\\n\\nDDPStrategy\\n\\nstatic_graph\\n\\nTrue\\n\\n))\\n\\nWhen Using DDP on a Multi-node Cluster, Set NCCL Parameters\\n\\nNCCL is the NVIDIA Collective Communications Library that is used by PyTorch to handle communication across nodes and GPUs. There are reported benefits in terms of speedups when adjusting NCCL parameters as seen in this issue. In the issue, we see a 30% speed improvement when training the Transformer XLM-RoBERTa and a 15% improvement in training with Detectron2.\\n\\nNCCL parameters can be adjusted via environment variables.\\n\\nNote\\n\\nAWS and GCP already set default values for these on their clusters. This is typically useful for custom cluster setups.\\n\\nNCCL_NSOCKS_PERTHREAD\\n\\nNCCL_SOCKET_NTHREADS\\n\\nNCCL_MIN_NCHANNELS\\n\\nexport\\n\\nNCCL_NSOCKS_PERTHREAD\\n\\nexport\\n\\nNCCL_SOCKET_NTHREADS\\n\\nGradients as Bucket View\\n\\nEnabling gradient_as_bucket_view=True in the DDPStrategy will make gradients views point to different offsets of the allreduce communication buckets. See DistributedDataParallel for more information.\\n\\nThis can reduce peak memory usage and throughput as saved memory will be equal to the total gradient memory + removes the need to copy gradients to the allreduce communication buckets.\\n\\nNote\\n\\nWhen gradient_as_bucket_view=True you cannot call detach_() on gradients. If hitting such errors, please fix it by referring to the zero_grad() function in torch/optim/optimizer.py as a solution (source).\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDDPStrategy\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDDPStrategy\\n\\ngradient_as_bucket_view\\n\\nTrue\\n\\n))\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nDDP Communication Hooks\\n\\nDDP Communication hooks is an interface to control how gradients are communicated across workers, overriding the standard allreduce in DistributedDataParallel. This allows you to enable performance improving communication hooks when using multiple nodes.\\n\\nEnable FP16 Compress Hook for multi-node throughput improvement:\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDDPStrategy\\n\\nfrom\\n\\ntorch.distributed.algorithms.ddp_comm_hooks\\n\\nimport\\n\\ndefault_hooks\\n\\nas\\n\\ndefault\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDDPStrategy\\n\\nddp_comm_hook\\n\\ndefault\\n\\nfp16_compress_hook\\n\\n))\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nEnable PowerSGD for multi-node throughput improvement:\\n\\nNote\\n\\nPowerSGD typically requires extra memory of the same size as the model’s gradients to enable error feedback, which can compensate for biased compressed communication and improve accuracy (source).\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDDPStrategy\\n\\nfrom\\n\\ntorch.distributed.algorithms.ddp_comm_hooks\\n\\nimport\\n\\npowerSGD_hook\\n\\nas\\n\\npowerSGD\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDDPStrategy\\n\\nddp_comm_state\\n\\npowerSGD\\n\\nPowerSGDState\\n\\nprocess_group\\n\\nNone\\n\\nmatrix_approximation_rank\\n\\nstart_powerSGD_iter\\n\\n5000\\n\\n),\\n\\nddp_comm_hook\\n\\npowerSGD\\n\\npowerSGD_hook\\n\\n),\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nCombine hooks for accumulated benefit:\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDDPStrategy\\n\\nfrom\\n\\ntorch.distributed.algorithms.ddp_comm_hooks\\n\\nimport\\n\\ndefault_hooks\\n\\nas\\n\\ndefault\\n\\npowerSGD_hook\\n\\nas\\n\\npowerSGD\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDDPStrategy\\n\\nddp_comm_state\\n\\npowerSGD\\n\\nPowerSGDState\\n\\nprocess_group\\n\\nNone\\n\\nmatrix_approximation_rank\\n\\nstart_powerSGD_iter\\n\\n5000\\n\\n),\\n\\nddp_comm_hook\\n\\npowerSGD\\n\\npowerSGD_hook\\n\\nddp_comm_wrapper\\n\\ndefault\\n\\nfp16_compress_wrapper\\n\\n),\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nWhen using Post-localSGD, you must also pass model_averaging_period to allow for model parameter averaging:\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDDPStrategy\\n\\nfrom\\n\\ntorch.distributed.algorithms.ddp_comm_hooks\\n\\nimport\\n\\npost_localSGD_hook\\n\\nas\\n\\npost_localSGD\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDDPStrategy\\n\\nddp_comm_state\\n\\npost_localSGD\\n\\nPostLocalSGDState\\n\\nprocess_group\\n\\nNone\\n\\nsubgroup\\n\\nNone\\n\\nstart_localSGD_iter\\n\\n),\\n\\nddp_comm_hook\\n\\npost_localSGD\\n\\npost_localSGD_hook\\n\\nmodel_averaging_period\\n\\n),\\n\\ntrainer\\n\\nfit\\n\\nmodel', metadata={'source': 'pl-docs/Train 1 trillion+ parameter models — PyTorch Lightning 2.1.0dev documentation.html'})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"mixed precision\"\n",
    "result = db.similarity_search(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='N-Bit Precision (Basic)\\n\\nAudience: Users looking to train models faster and consume less memory.\\n\\nIf you’re looking to run models faster or consume less memory, consider tweaking the precision settings of your models.\\n\\nLower precision, such as 16-bit floating-point, requires less memory and enables training and deploying larger models.\\nHigher precision, such as the 64-bit floating-point, can be used for highly sensitive use-cases.\\n\\n16-bit Precision\\n\\nUse 16-bit mixed precision to lower your memory consumption by up to half so that you can train and deploy larger models. If your GPUs are [Tensor Core] GPUs, you can also get a ~3x speed improvement. Half precision can sometimes lead to unstable training.\\n\\nTrainer\\n\\nprecision\\n\\n\\'16-mixed\\'\\n\\n32-bit Precision\\n\\n32-bit precision is the default used across all models and research. This precision is known to be stable in contrast to lower precision settings.\\n\\nTrainer\\n\\nprecision\\n\\n\"32-true\"\\n\\n# or\\n\\nTrainer\\n\\nprecision\\n\\n\"32\"\\n\\n# or\\n\\nTrainer\\n\\nprecision\\n\\n32\\n\\n64-bit Precision\\n\\nFor certain scientific computations, 64-bit precision enables more accurate models. However, doubling the precision from 32 to 64 bit also doubles the memory requirements.\\n\\nTrainer\\n\\nprecision\\n\\n\"64-true\"\\n\\n# or\\n\\nTrainer\\n\\nprecision\\n\\n\"64\"\\n\\n# or\\n\\nTrainer\\n\\nprecision\\n\\n64\\n\\nNote\\n\\nSince in deep learning, memory is always a bottleneck, especially when dealing with a large volume of data and with limited resources.\\nIt is recommended using single precision for better speed. Although you can still use it if you want for your particular use-case.\\n\\nPrecision support by accelerator\\n\\nPrecision with Accelerators\\n\\nPrecision\\n\\nCPU\\n\\nGPU\\n\\nTPU\\n\\nIPU\\n\\n16 Mixed\\n\\nNo\\n\\nYes\\n\\nNo\\n\\nYes\\n\\nBFloat16 Mixed\\n\\nYes\\n\\nYes\\n\\nYes\\n\\nNo\\n\\n32 True\\n\\nYes\\n\\nYes\\n\\nYes\\n\\nYes\\n\\n64 True\\n\\nYes\\n\\nYes\\n\\nNo\\n\\nNo', metadata={'source': 'pl-docs/N-Bit Precision (Basic) — PyTorch Lightning 2.1.0dev documentation.html'}),\n",
       " Document(page_content='Installation\\n\\nApple Silicon (M1/M2/M3) Macs\\n\\nWhile ML related python packages are updated to work with Apple Silicon, you’ll need to set 2 environment variables on install.\\n\\n# needed for M1/M2/M3\\n\\nexport\\n\\nGRPC_PYTHON_BUILD_SYSTEM_OPENSSL\\n\\nexport\\n\\nGRPC_PYTHON_BUILD_SYSTEM_ZLIB\\n\\n1\\n\\npython\\n\\nm\\n\\npip\\n\\ninstall\\n\\nU\\n\\nlightning\\n\\nInstall with pip\\n\\nInstall lightning inside a virtual env or conda environment with pip\\n\\npython\\n\\nm\\n\\npip\\n\\ninstall\\n\\nlightning\\n\\nInstall with Conda\\n\\nIf you don’t have conda installed, follow the Conda Installation Guide.\\nLightning can be installed with conda using the following command:\\n\\nconda\\n\\ninstall\\n\\nlightning\\n\\nc\\n\\nconda-forge\\n\\nYou can also use Conda Environments:\\n\\nconda\\n\\nactivate\\n\\nmy_env\\nconda\\n\\ninstall\\n\\nlightning\\n\\nc\\n\\nconda-forge\\n\\nBuild from Source\\n\\nInstall nightly from the source. Note that it contains all the bug fixes and newly released features that\\nare not published yet. This is the bleeding edge, so use it at your own discretion.\\n\\npip\\n\\ninstall\\n\\nhttps://github.com/Lightning-AI/lightning/archive/refs/heads/master.zip\\n\\nU\\n\\nInstall future patch releases from the source. Note that the patch release contains only the bug fixes for the recent major release.\\n\\npip\\n\\ninstall\\n\\nhttps://github.com/Lightning-AI/lightning/archive/refs/heads/release/stable.zip\\n\\nU\\n\\nOptimized for model development\\n\\nIf you are deploying models built with Lightning in production and require few dependencies, try using the optimized lightning[pytorch] package:\\n\\npip\\n\\ninstall\\n\\nlightning\\n\\nCustom PyTorch Version\\n\\nTo use any PyTorch version visit the PyTorch Installation Page.\\n\\nOptimized for ML workflows (lightning Apps)\\n\\nIf you are deploying workflows built with Lightning in production and require fewer dependencies, try using the optimized lightning[apps] package:\\n\\npip\\n\\ninstall\\n\\nlightning-app', metadata={'source': 'pl-docs/Installation — PyTorch Lightning 2.1.0dev documentation.html'}),\n",
       " Document(page_content='Effective Training Techniques\\n\\nLightning implements various techniques to help during training that can help make the training smoother.\\n\\nAccumulate Gradients\\n\\nAccumulated gradients run K small batches of size N before doing a backward pass. The effect is a large effective batch size of size KxN, where N is the batch size.\\nInternally it doesn’t stack up the batches and do a forward pass rather it accumulates the gradients for K batches and then do an optimizer.step to make sure the\\neffective batch size is increased but there is no memory overhead.\\n\\nWarning\\n\\nloss.backward()\\n\\noptimizer.step()\\n\\nN*K\\n\\noptimizer.step()\\n\\nP*N*K\\n\\nN*K\\n\\n# DEFAULT (ie: no accumulated grads)\\n\\ntrainer\\n\\nTrainer\\n\\naccumulate_grad_batches\\n\\n# Accumulate gradients for 7 batches\\n\\ntrainer\\n\\nTrainer\\n\\naccumulate_grad_batches\\n\\nOptionally, you can make the accumulate_grad_batches value change over time by using the GradientAccumulationScheduler.\\nPass in a scheduling dictionary, where the key represents the epoch at which the value for gradient accumulation should be updated.\\n\\nfrom\\n\\nlightning.pytorch.callbacks\\n\\nimport\\n\\nGradientAccumulationScheduler\\n\\n# till 5th epoch, it will accumulate every 8 batches. From 5th epoch\\n\\n# till 9th epoch it will accumulate every 4 batches and after that no accumulation\\n\\n# will happen. Note that you need to use zero-indexed epoch keys here\\n\\naccumulator\\n\\nGradientAccumulationScheduler\\n\\nscheduling\\n\\n})\\n\\ntrainer\\n\\nTrainer\\n\\ncallbacks\\n\\naccumulator\\n\\nNote: Not all strategies and accelerators support variable gradient accumulation windows.\\n\\nGradient Clipping\\n\\nGradient clipping can be enabled to avoid exploding gradients. By default, this will clip the gradient norm by calling\\ntorch.nn.utils.clip_grad_norm_() computed over all model parameters together.\\nIf the Trainer’s gradient_clip_algorithm is set to \\'value\\' (\\'norm\\' by default), this will use instead\\ntorch.nn.utils.clip_grad_value_() for each parameter instead.\\n\\nNote\\n\\nIf using mixed precision, the gradient_clip_val does not need to be changed as the gradients are unscaled\\nbefore applying the clipping function.\\n\\nSee also\\n\\nTrainer\\n\\n# DEFAULT (ie: don\\'t clip)\\n\\ntrainer\\n\\nTrainer\\n\\ngradient_clip_val\\n\\n# clip gradients\\' global norm to <=0.5 using gradient_clip_algorithm=\\'norm\\' by default\\n\\ntrainer\\n\\nTrainer\\n\\ngradient_clip_val\\n\\n0.5\\n\\n# clip gradients\\' maximum magnitude to <=0.5\\n\\ntrainer\\n\\nTrainer\\n\\ngradient_clip_val\\n\\n0.5\\n\\ngradient_clip_algorithm\\n\\n\"value\"\\n\\nRead more about Configuring Gradient Clipping for advanced use-cases.\\n\\nStochastic Weight Averaging\\n\\nStochastic Weight Averaging (SWA) can make your models generalize better at virtually no additional cost.\\nThis can be used with both non-trained and trained models. The SWA procedure smooths the loss landscape thus making\\nit harder to end up in a local minimum during optimization.\\n\\nFor a more detailed explanation of SWA and how it works,\\nread this post by the PyTorch team.\\n\\nSee also\\n\\nThe StochasticWeightAveraging callback\\n\\n# Enable Stochastic Weight Averaging using the callback\\n\\ntrainer\\n\\nTrainer\\n\\ncallbacks\\n\\nStochasticWeightAveraging\\n\\nswa_lrs\\n\\n1e-2\\n\\n)])\\n\\nBatch Size Finder\\n\\nAuto-scaling of batch size can be enabled to find the largest batch size that fits into\\nmemory. Large batch size often yields a better estimation of the gradients, but may also result in\\nlonger training time. Inspired by https://github.com/BlackHC/toma.\\n\\nSee also\\n\\nTuner\\n\\nfrom\\n\\nlightning.pytorch.tuner\\n\\nimport\\n\\nTuner\\n\\n# Create a tuner for the trainer\\n\\ntrainer\\n\\nTrainer\\n\\n...\\n\\ntuner\\n\\nTuner\\n\\ntrainer\\n\\n# Auto-scale batch size by growing it exponentially (default)\\n\\ntuner\\n\\nscale_batch_size\\n\\nmodel\\n\\nmode\\n\\n\"power\"\\n\\n# Auto-scale batch size with binary search\\n\\ntuner\\n\\nscale_batch_size\\n\\nmodel\\n\\nmode\\n\\n\"binsearch\"\\n\\n# Fit as normal with new batch size\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nCurrently, this feature supports two modes \\'power\\' scaling and \\'binsearch\\'\\nscaling. In \\'power\\' scaling, starting from a batch size of 1 keeps doubling\\nthe batch size until an out-of-memory (OOM) error is encountered. Setting the\\nargument to \\'binsearch\\' will initially also try doubling the batch size until\\nit encounters an OOM, after which it will do a binary search that will finetune the\\nbatch size. Additionally, it should be noted that the batch size scaler cannot\\nsearch for batch sizes larger than the size of the training dataset.\\n\\nNote\\n\\nThis feature expects that a batch_size field is either located as a model attribute\\ni.e. model.batch_size or as a field in your hparams i.e. model.hparams.batch_size.\\nSimilarly it can work with datamodules too. The field should exist and will be updated by\\nthe results of this algorithm. Additionally, your train_dataloader() method should depend\\non this field for this feature to work i.e.\\n\\n# using LightningModule\\n\\nclass\\n\\nLitModel\\n\\nLightningModule\\n\\n):\\n\\ndef\\n\\n__init__\\n\\nself\\n\\nbatch_size\\n\\n):\\n\\nsuper\\n\\n()\\n\\n__init__\\n\\n()\\n\\nself\\n\\nsave_hyperparameters\\n\\n()\\n\\n# or\\n\\nself\\n\\nbatch_size\\n\\nbatch_size\\n\\ndef\\n\\ntrain_dataloader\\n\\nself\\n\\n):\\n\\nreturn\\n\\nDataLoader\\n\\ntrain_dataset\\n\\nbatch_size\\n\\nself\\n\\nbatch_size\\n\\nself\\n\\nhparams\\n\\nbatch_size\\n\\nmodel\\n\\nLitModel\\n\\nbatch_size\\n\\n32\\n\\ntrainer\\n\\nTrainer\\n\\n...\\n\\ntuner\\n\\nTuner\\n\\ntrainer\\n\\ntuner\\n\\nscale_batch_size\\n\\nmodel\\n\\n# using LightningDataModule\\n\\nclass\\n\\nLitDataModule\\n\\nLightningDataModule\\n\\n):\\n\\ndef\\n\\n__init__\\n\\nself\\n\\nbatch_size\\n\\n):\\n\\nsuper\\n\\n()\\n\\n__init__\\n\\n()\\n\\nself\\n\\nsave_hyperparameters\\n\\n()\\n\\n# or\\n\\nself\\n\\nbatch_size\\n\\nbatch_size\\n\\ndef\\n\\ntrain_dataloader\\n\\nself\\n\\n):\\n\\nreturn\\n\\nDataLoader\\n\\ntrain_dataset\\n\\nbatch_size\\n\\nself\\n\\nbatch_size\\n\\nself\\n\\nhparams\\n\\nbatch_size\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ndatamodule\\n\\nLitDataModule\\n\\nbatch_size\\n\\n32\\n\\ntrainer\\n\\nTrainer\\n\\n...\\n\\ntuner\\n\\nTuner\\n\\ntrainer\\n\\ntuner\\n\\nscale_batch_size\\n\\nmodel\\n\\ndatamodule\\n\\ndatamodule\\n\\ntrain_dataloader\\n\\nLightningModule\\n\\nLightningDataModule\\n\\nLightningModule\\n\\nLightningDataModule\\n\\ntrain_dataloader\\n\\nLightningDataModule\\n\\nDumping the current state of the model and trainer\\n\\nIteratively until convergence or maximum number of tries max_trials (default 25) has been reached:\\nCall fit() method of trainer. This evaluates steps_per_trial (default 3) number of\\noptimization steps. Each training step can trigger an OOM error if the tensors\\n(training batch, weights, gradients, etc.) allocated during the steps have a\\ntoo large memory footprint.\\nIf an OOM error is encountered, decrease batch size else increase it.\\nHow much the batch size is increased/decreased is determined by the chosen\\nstrategy.\\n\\n\\n\\n\\nThe found batch size is saved to either model.batch_size or model.hparams.batch_size\\nRestore the initial state of model and trainer\\n\\nWarning\\n\\nBatch size finder is not yet supported for DDP or any of its variations, it is coming soon.\\n\\nCustomizing Batch Size Finder\\n\\nWarning\\n\\nThis is an experimental feature.\\n\\nYou can also customize the BatchSizeFinder callback to run\\nat different epochs. This feature is useful while fine-tuning models since you can’t always use the same batch size after\\nunfreezing the backbone.\\n\\nfrom\\n\\nlightning.pytorch.callbacks\\n\\nimport\\n\\nBatchSizeFinder\\n\\nclass\\n\\nFineTuneBatchSizeFinder\\n\\nBatchSizeFinder\\n\\n):\\n\\ndef\\n\\n__init__\\n\\nself\\n\\nmilestones\\n\\nargs\\n\\n*\\n\\nkwargs\\n\\n):\\n\\nsuper\\n\\n()\\n\\n__init__\\n\\nargs\\n\\n*\\n\\nkwargs\\n\\nself\\n\\nmilestones\\n\\nmilestones\\n\\ndef\\n\\non_fit_start\\n\\nself\\n\\nargs\\n\\n*\\n\\nkwargs\\n\\n):\\n\\nreturn\\n\\ndef\\n\\non_train_epoch_start\\n\\nself\\n\\ntrainer\\n\\npl_module\\n\\n):\\n\\nif\\n\\ntrainer\\n\\ncurrent_epoch\\n\\nin\\n\\nself\\n\\nmilestones\\n\\nor\\n\\ntrainer\\n\\ncurrent_epoch\\n\\n==\\n\\nself\\n\\nscale_batch_size\\n\\ntrainer\\n\\npl_module\\n\\ntrainer\\n\\nTrainer\\n\\ncallbacks\\n\\nFineTuneBatchSizeFinder\\n\\nmilestones\\n\\n10\\n\\n))])\\n\\ntrainer\\n\\nfit\\n\\n...\\n\\nRun batch size finder for validate/test/predict.\\n\\nfrom\\n\\nlightning.pytorch.callbacks\\n\\nimport\\n\\nBatchSizeFinder\\n\\nclass\\n\\nEvalBatchSizeFinder\\n\\nBatchSizeFinder\\n\\n):\\n\\ndef\\n\\n__init__\\n\\nself\\n\\nargs\\n\\n*\\n\\nkwargs\\n\\n):\\n\\nsuper\\n\\n()\\n\\n__init__\\n\\nargs\\n\\n*\\n\\nkwargs\\n\\ndef\\n\\non_fit_start\\n\\nself\\n\\nargs\\n\\n*\\n\\nkwargs\\n\\n):\\n\\nreturn\\n\\ndef\\n\\non_test_start\\n\\nself\\n\\ntrainer\\n\\npl_module\\n\\n):\\n\\nself\\n\\nscale_batch_size\\n\\ntrainer\\n\\npl_module\\n\\ntrainer\\n\\nTrainer\\n\\ncallbacks\\n\\nEvalBatchSizeFinder\\n\\n()])\\n\\ntrainer\\n\\ntest\\n\\n...\\n\\nLearning Rate Finder\\n\\nFor training deep neural networks, selecting a good learning rate is essential\\nfor both better performance and faster convergence. Even optimizers such as\\nAdam that are self-adjusting the learning rate can benefit from more optimal\\nchoices.\\n\\nTo reduce the amount of guesswork concerning choosing a good initial learning\\nrate, a learning rate finder can be used. As described in this paper\\na learning rate finder does a small run where the learning rate is increased\\nafter each processed batch and the corresponding loss is logged. The result of\\nthis is a lr vs. loss plot that can be used as guidance for choosing an optimal\\ninitial learning rate.\\n\\nWarning\\n\\nFor the moment, this feature only works with models having a single optimizer.\\n\\nNote\\n\\nWith DDP: Since all the processes run in isolation, only process with global_rank=0 will make the decision to stop the\\nlearning rate finder and broadcast its results to all other ranks. That means, at the end of LR finder, each process will be running with\\nthe learning rate found on global_rank=0.\\n\\nUsing Lightning’s built-in LR finder\\n\\nlightning module needs to\\nhave a\\n\\nlearning_rate\\n\\nlr\\n\\nhparams\\n\\nhparams.learning_rate\\n\\nhparams.lr\\n\\nTuner via\\n\\ntuner\\n\\nTuner(trainer)\\n\\ntuner.lr_find(model)\\n\\nlearning_rate\\n\\nlightning module, which can be accessed\\nvia\\n\\nself.learning_rate\\n\\nself.lr\\n\\nfrom\\n\\nlightning.pytorch.tuner\\n\\nimport\\n\\nTuner\\n\\nclass\\n\\nLitModel\\n\\nLightningModule\\n\\n):\\n\\ndef\\n\\n__init__\\n\\nself\\n\\nlearning_rate\\n\\n):\\n\\nsuper\\n\\n()\\n\\n__init__\\n\\n()\\n\\nself\\n\\nlearning_rate\\n\\nlearning_rate\\n\\nself\\n\\nmodel\\n\\nModel\\n\\n...\\n\\ndef\\n\\nconfigure_optimizers\\n\\nself\\n\\n):\\n\\nreturn\\n\\nAdam\\n\\nself\\n\\nparameters\\n\\n(),\\n\\nlr\\n\\nself\\n\\nlr\\n\\nor\\n\\nself\\n\\nlearning_rate\\n\\n))\\n\\nmodel\\n\\nLitModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\n...\\n\\n# Create a Tuner\\n\\ntuner\\n\\nTuner\\n\\ntrainer\\n\\n# finds learning rate automatically\\n\\n# sets hparams.lr or hparams.learning_rate to that learning rate\\n\\ntuner\\n\\nlr_find\\n\\nmodel\\n\\nIf your model is using an arbitrary value instead of self.lr or self.learning_rate, set that value in lr_find:\\n\\nmodel\\n\\nLitModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\n...\\n\\ntuner\\n\\nTuner\\n\\ntrainer\\n\\n# to set to your own hparams.my_value\\n\\ntuner\\n\\nlr_find\\n\\nmodel\\n\\nattr_name\\n\\n\"my_value\"\\n\\nYou can also inspect the results of the learning rate finder or just play around\\nwith the parameters of the algorithm. A typical example of this would look like:\\n\\nmodel\\n\\nMyModelClass\\n\\nhparams\\n\\ntrainer\\n\\nTrainer\\n\\n()\\n\\ntuner\\n\\nTuner\\n\\ntrainer\\n\\n# Run learning rate finder\\n\\nlr_finder\\n\\ntuner\\n\\nlr_find\\n\\nmodel\\n\\n# Results can be found in\\n\\nprint\\n\\nlr_finder\\n\\nresults\\n\\n# Plot with\\n\\nfig\\n\\nlr_finder\\n\\nplot\\n\\nsuggest\\n\\nTrue\\n\\nfig\\n\\nshow\\n\\n()\\n\\n# Pick point based on plot, or get suggestion\\n\\nnew_lr\\n\\nlr_finder\\n\\nsuggestion\\n\\n()\\n\\n# update hparams of the model\\n\\nmodel\\n\\nhparams\\n\\nlr\\n\\nnew_lr\\n\\n# Fit model\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nThe figure produced by lr_finder.plot() should look something like the figure\\nbelow. It is recommended to not pick the learning rate that achieves the lowest\\nloss, but instead something in the middle of the sharpest downward slope (red point).\\nThis is the point returned py lr_finder.suggestion().\\n\\nCustomizing Learning Rate Finder\\n\\nWarning\\n\\nThis is an experimental feature.\\n\\nYou can also customize the LearningRateFinder callback to run at different epochs. This feature is useful while fine-tuning models.\\n\\nfrom\\n\\nlightning.pytorch.callbacks\\n\\nimport\\n\\nLearningRateFinder\\n\\nclass\\n\\nFineTuneLearningRateFinder\\n\\nLearningRateFinder\\n\\n):\\n\\ndef\\n\\n__init__\\n\\nself\\n\\nmilestones\\n\\nargs\\n\\n*\\n\\nkwargs\\n\\n):\\n\\nsuper\\n\\n()\\n\\n__init__\\n\\nargs\\n\\n*\\n\\nkwargs\\n\\nself\\n\\nmilestones\\n\\nmilestones\\n\\ndef\\n\\non_fit_start\\n\\nself\\n\\nargs\\n\\n*\\n\\nkwargs\\n\\n):\\n\\nreturn\\n\\ndef\\n\\non_train_epoch_start\\n\\nself\\n\\ntrainer\\n\\npl_module\\n\\n):\\n\\nif\\n\\ntrainer\\n\\ncurrent_epoch\\n\\nin\\n\\nself\\n\\nmilestones\\n\\nor\\n\\ntrainer\\n\\ncurrent_epoch\\n\\n==\\n\\nself\\n\\nlr_find\\n\\ntrainer\\n\\npl_module\\n\\ntrainer\\n\\nTrainer\\n\\ncallbacks\\n\\nFineTuneLearningRateFinder\\n\\nmilestones\\n\\n10\\n\\n))])\\n\\ntrainer\\n\\nfit\\n\\n...\\n\\nAdvanced GPU Optimizations\\n\\nWhen training on single or multiple GPU machines, Lightning offers a host of advanced optimizations to improve throughput, memory efficiency, and model scaling.\\nRefer to Advanced GPU Optimized Training for more details.\\n\\nSharing Datasets Across Process Boundaries\\n\\nThe LightningDataModule class provides an organized way to decouple data loading from training logic, with prepare_data() being used for downloading and pre-processing the dataset on a single process, and setup() loading the pre-processed data for each process individually:\\n\\nclass\\n\\nMNISTDataModule\\n\\npl\\n\\nLightningDataModule\\n\\n):\\n\\ndef\\n\\nprepare_data\\n\\nself\\n\\n):\\n\\nMNIST\\n\\nself\\n\\ndata_dir\\n\\ndownload\\n\\nTrue\\n\\ndef\\n\\nsetup\\n\\nself\\n\\nstage\\n\\nstr\\n\\n):\\n\\nself\\n\\nmnist\\n\\nMNIST\\n\\nself\\n\\ndata_dir\\n\\ndef\\n\\ntrain_loader\\n\\nself\\n\\n):\\n\\nreturn\\n\\nDataLoader\\n\\nself\\n\\nmnist\\n\\nbatch_size\\n\\n128\\n\\nHowever, for in-memory datasets, that means that each process will hold a (redundant) replica of the dataset in memory, which may be impractical when using many processes while utilizing datasets that nearly fit into CPU memory, as the memory consumption will scale up linearly with the number of processes.\\nFor example, when training Graph Neural Networks, a common strategy is to load the entire graph into CPU memory for fast access to the entire graph structure and its features, and to then perform neighbor sampling to obtain mini-batches that fit onto the GPU.\\n\\nA simple way to prevent redundant dataset replicas is to rely on torch.multiprocessing to share the data automatically between spawned processes via shared memory.\\nFor this, all data pre-loading should be done on the main process inside DataModule.__init__(). As a result, all tensor-data will get automatically shared when using the \\'ddp_spawn\\' strategy.\\n\\nWarning\\n\\ntorch.multiprocessing will send a handle of each individual tensor to other processes.\\nIn order to prevent any errors due to too many open file handles, try to reduce the number of tensors to share, e.g., by stacking your data into a single tensor.\\n\\nclass\\n\\nMNISTDataModule\\n\\npl\\n\\nLightningDataModule\\n\\n):\\n\\ndef\\n\\n__init__\\n\\nself\\n\\ndata_dir\\n\\nstr\\n\\n):\\n\\nself\\n\\nmnist\\n\\nMNIST\\n\\ndata_dir\\n\\ndownload\\n\\nTrue\\n\\ntransform\\n\\nToTensor\\n\\n())\\n\\ndef\\n\\ntrain_loader\\n\\nself\\n\\n):\\n\\nreturn\\n\\nDataLoader\\n\\nself\\n\\nmnist\\n\\nbatch_size\\n\\n128\\n\\nmodel\\n\\nModel\\n\\n...\\n\\ndatamodule\\n\\nMNISTDataModule\\n\\n\"data/MNIST\"\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"ddp_spawn\"\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\ndatamodule\\n\\nSee the graph-level and node-level prediction examples in PyTorch Geometric for practical use-cases.', metadata={'source': 'pl-docs/Effective Training Techniques — PyTorch Lightning 2.1.0dev documentation.html'}),\n",
       " Document(page_content='Train 1 trillion+ parameter models\\n\\nWhen training large models, fitting larger batch sizes, or trying to increase throughput using multi-GPU compute, Lightning provides advanced optimized distributed training strategies to support these cases and offer substantial improvements in memory usage.\\n\\nNote that some of the extreme memory saving configurations will affect the speed of training. This Speed/Memory trade-off in most cases can be adjusted.\\n\\nSome of these memory-efficient strategies rely on offloading onto other forms of memory, such as CPU RAM or NVMe. This means you can even see memory benefits on a single GPU, using a strategy such as DeepSpeed ZeRO Stage 3 Offload.\\n\\nCheck out this amazing video explaining model parallelism and how it works behind the scenes:\\n\\nChoosing an Advanced Distributed GPU Strategy\\n\\nIf you would like to stick with PyTorch DDP, see DDP Optimizations.\\n\\nUnlike DistributedDataParallel (DDP) where the maximum trainable model size and batch size do not change with respect to the number of GPUs, memory-optimized strategies can accommodate bigger models and larger batches as more GPUs are used. This means as you scale up the number of GPUs, you can reach the number of model parameters you’d like to train.\\n\\nThere are many considerations when choosing a strategy as described below. In addition, check out the visualization of various strategy benchmarks using minGPT here.\\n\\nPre-training vs Fine-tuning\\n\\nWhen fine-tuning, we often use a magnitude less data compared to pre-training a model. This is important when choosing a distributed strategy as usually for pre-training, we are compute-bound.\\nThis means we cannot sacrifice throughput as much as if we were fine-tuning, because in fine-tuning the data requirement is smaller.\\n\\nOverall:\\n\\nWhen fine-tuning a model, use advanced memory efficient strategies such as Fully Sharded Training, DeepSpeed ZeRO Stage 3 or DeepSpeed ZeRO Stage 3 Offload, allowing you to fine-tune larger models if you are limited on compute\\n\\nWhen pre-training a model, use simpler optimizations such as DeepSpeed ZeRO Stage 2, scaling the number of GPUs to reach larger parameter sizes\\n\\nFor both fine-tuning and pre-training, use DeepSpeed Activation Checkpointing as the throughput degradation is not significant\\n\\nFor example when using 128 GPUs, you can pre-train large 10 to 20 Billion parameter models using DeepSpeed ZeRO Stage 2 without having to take a performance hit with more advanced optimized multi-gpu strategy.\\n\\nBut for fine-tuning a model, you can reach 10 to 20 Billion parameter models using DeepSpeed ZeRO Stage 3 Offload on a single GPU. This does come with a significant throughput hit, which needs to be weighed accordingly.\\n\\nWhen Shouldn’t I use an Optimized Distributed Strategy?\\n\\nSharding techniques help when model sizes are fairly large; roughly 500M+ parameters is where we’ve seen benefits. However, in the following cases, we recommend sticking to ordinary distributed strategies\\n\\nWhen your model is small (ResNet50 of around 80M Parameters), unless you are using unusually large batch sizes or inputs.\\n\\nDue to high distributed communication between devices, if running on a slow network/interconnect, the training might be much slower than expected and then it’s up to you to determince the tradeoff here.\\n\\nCutting-edge and third-party Strategies\\n\\nCutting-edge Lightning strategies are being developed by third-parties outside of Lightning.\\n\\nIf you want to try some of the latest and greatest features for model-parallel training, check out the Colossal-AI Strategy integration.\\n\\nAnother integration is Bagua Strategy, deep learning training acceleration framework for PyTorch, with advanced distributed training algorithms and system optimizations.\\n\\nFor training on unreliable mixed GPUs across the internet check out the Hivemind Strategy integration.\\n\\nFully Sharded Training\\n\\nPyTorch has it’s own version of FSDP which is upstreamed from their fairscale project.\\nIt was introduced in their v1.11.0 release but it is recommended to use it with PyTorch v1.12 or more and that’s what\\nLightning supports.\\n\\nWarning\\n\\nThis is an experimental feature.\\n\\nAuto Wrapping\\n\\nModel layers should be wrapped in FSDP in a nested way to save peak memory and enable communication and computation overlapping. The\\nsimplest way to do it is auto wrapping, which can serve as a drop-in replacement for DDP without changing the rest of the code. You don’t\\nhave to wrap layers manually as in the case of manual wrapping.\\n\\nNote\\n\\nWhile initializing the optimizers inside configure_optimizers hook, make sure to use self.trainer.model.parameters(), else\\nPyTorch will raise an error. This is required because when you use auto-wrap, the model layers are sharded and your\\nlightning_module.parameters() will return a generator with no params. This inconvenience will be addressed in the future.\\n\\nmodel\\n\\nBoringModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"fsdp\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nRead more here.\\n\\nManual Wrapping\\n\\nManual wrapping can be useful to explore complex sharding strategies by applying wrap selectively to some parts of the model. To activate\\nparameter sharding with manual wrapping, you can wrap your model using the wrap function. Internally in Lightning, we enable a context manager around the configure_sharded_model function to make sure the wrap parameters are passed correctly.\\n\\nWhen not using Fully Sharded these wrap functions are a no-op. This means once the changes have been made, there is no need to remove the changes for other strategies.\\n\\nwrap simply wraps the module with a Fully Sharded Parallel class with the correct parameters from the Lightning context manager.\\n\\nHere’s an example using that uses wrap to create your model:\\n\\nimport\\n\\ntorch\\n\\nimport\\n\\ntorch.nn\\n\\nas\\n\\nnn\\n\\nimport\\n\\nlightning.pytorch\\n\\nas\\n\\npl\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\ntorch.distributed.fsdp.wrap\\n\\nimport\\n\\nwrap\\n\\nclass\\n\\nMyModel\\n\\npl\\n\\nLightningModule\\n\\n):\\n\\ndef\\n\\n__init__\\n\\nself\\n\\n):\\n\\nsuper\\n\\n()\\n\\n__init__\\n\\n()\\n\\nself\\n\\nlinear_layer\\n\\nnn\\n\\nLinear\\n\\n32\\n\\n32\\n\\nself\\n\\nblock\\n\\nnn\\n\\nSequential\\n\\nnn\\n\\nLinear\\n\\n32\\n\\n32\\n\\n),\\n\\nnn\\n\\nLinear\\n\\n32\\n\\n32\\n\\n))\\n\\ndef\\n\\nconfigure_sharded_model\\n\\nself\\n\\n):\\n\\n# modules are sharded across processes\\n\\n# as soon as they are wrapped with `wrap`.\\n\\n# During the forward/backward passes, weights get synced across processes\\n\\n# and de-allocated once computation is complete, saving memory.\\n\\n# Wraps the layer in a Fully Sharded Wrapper automatically\\n\\nlinear_layer\\n\\nwrap\\n\\nself\\n\\nlinear_layer\\n\\nfor\\n\\nlayer\\n\\nin\\n\\nenumerate\\n\\nself\\n\\nblock\\n\\n):\\n\\nself\\n\\nblock\\n\\nwrap\\n\\nlayer\\n\\nself\\n\\nmodel\\n\\nnn\\n\\nSequential\\n\\nlinear_layer\\n\\nnn\\n\\nReLU\\n\\n(),\\n\\nself\\n\\nblock\\n\\ndef\\n\\nconfigure_optimizers\\n\\nself\\n\\n):\\n\\nreturn\\n\\ntorch\\n\\noptim\\n\\nAdamW\\n\\nself\\n\\nmodel\\n\\nparameters\\n\\n())\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"fsdp\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nYou can customize the strategy configuration by adjusting the arguments of FSDPStrategy and pass that to the strategy argument inside the Trainer.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nFSDPStrategy\\n\\nfsdp\\n\\nFSDPStrategy\\n\\ncpu_offload\\n\\nTrue\\n\\n# equivalent to passing `\"fsdp_cpu_offload\"`\\n\\ntrainer\\n\\npl\\n\\nTrainer\\n\\nstrategy\\n\\nfsdp\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nCheck out this tutorial to learn more about it.\\n\\nActivation Checkpointing\\n\\nActivation checkpointing reduces GPU memory usage by avoiding the storage of intermediate activation tensors in\\nselected layers. The tradeoff is that computation cost for the backpropagation increases, as the dropped activations\\nneed to be recomputed.\\n\\nEnable checkpointing on large layers (like Transformers) by providing the layer class/type to the strategy:\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nFSDPStrategy\\n\\nfsdp\\n\\nFSDPStrategy\\n\\nactivation_checkpointing\\n\\nMyTransformerBlock\\n\\n# or pass a list with multiple types\\n\\ntrainer\\n\\npl\\n\\nTrainer\\n\\nstrategy\\n\\nfsdp\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nDeepSpeed\\n\\nDeepSpeed is a deep learning training optimization library, providing the means to train massive billion parameter models at scale.\\nUsing the DeepSpeed strategy, we were able to train model sizes of 10 Billion parameters and above, with a lot of useful information in this benchmark and the DeepSpeed docs.\\nDeepSpeed also offers lower level training optimizations, and efficient optimizers such as 1-bit Adam. We recommend using DeepSpeed in environments where speed and memory optimizations are important (such as training large billion parameter models).\\n\\nWarning\\n\\nThis is an experimental feature.\\n\\nBelow is a summary of all the configurations of DeepSpeed.\\n\\nDeepSpeed ZeRO Stage 1 - Shard optimizer states, remains at speed parity with DDP whilst providing memory improvement\\n\\nDeepSpeed ZeRO Stage 2 - Shard optimizer states and gradients, remains at speed parity with DDP whilst providing even more memory improvement\\n\\nDeepSpeed ZeRO Stage 2 Offload - Offload optimizer states and gradients to CPU. Increases distributed communication volume and GPU-CPU device transfer, but provides significant memory improvement\\n\\nDeepSpeed ZeRO Stage 3 - Shard optimizer states, gradients, parameters and optionally activations. Increases distributed communication volume, but provides even more memory improvement\\n\\nDeepSpeed ZeRO Stage 3 Offload - Offload optimizer states, gradients, parameters and optionally activations to CPU. Increases distributed communication volume and GPU-CPU device transfer, but even more significant memory improvement.\\n\\nDeepSpeed Activation Checkpointing - Free activations after forward pass. Increases computation, but provides memory improvement for all stages.\\n\\nTo use DeepSpeed, you first need to install DeepSpeed using the commands below.\\n\\npip\\n\\ninstall\\n\\ndeepspeed\\n\\nIf you run into an issue with the install or later in training, ensure that the CUDA version of the PyTorch you’ve installed matches your locally installed CUDA (you can see which one has been recognized by running nvcc --version).\\n\\nNote\\n\\nDeepSpeed currently only supports single optimizer, single scheduler within the training loop.\\n\\nWhen saving a checkpoint we rely on DeepSpeed which saves a directory containing the model and various components.\\n\\nDeepSpeed ZeRO Stage 1\\n\\nDeepSpeed ZeRO Stage 1 partitions your optimizer states (Stage 1) across your GPUs to reduce memory.\\n\\nIt is recommended to skip Stage 1 and use Stage 2, which comes with larger memory improvements and still remains efficient. Stage 1 is useful to pair with certain optimizations such as Torch ORT.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_1\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nDeepSpeed ZeRO Stage 2\\n\\nDeepSpeed ZeRO Stage 2 partitions your optimizer states (Stage 1) and your gradients (Stage 2) across your GPUs to reduce memory. In most cases, this is more efficient or at parity with DDP, primarily due to the optimized custom communications written by the DeepSpeed team.\\nAs a result, benefits can also be seen on a single GPU. Do note that the default bucket sizes allocate around 3.6GB of VRAM to use during distributed communications, which can be tweaked when instantiating the strategy described in a few sections below.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_2\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\npython\\n\\ntrain.py\\n\\n-strategy\\n\\ndeepspeed_stage_2\\n\\n-precision\\n\\n16\\n\\n-accelerator\\n\\n\\'gpu\\'\\n\\n-devices\\n\\nDeepSpeed ZeRO Stage 2 Offload\\n\\nBelow we show an example of running ZeRO-Offload. ZeRO-Offload leverages the host CPU to offload optimizer memory/computation, reducing the overall memory consumption.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_2_offload\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nThis can also be done via the command line using a PyTorch Lightning script:\\n\\npython\\n\\ntrain.py\\n\\n-strategy\\n\\ndeepspeed_stage_2_offload\\n\\n-precision\\n\\n16\\n\\n-accelerator\\n\\n\\'gpu\\'\\n\\n-devices\\n\\nYou can also modify the ZeRO-Offload parameters via the strategy as below.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDeepSpeedStrategy\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDeepSpeedStrategy\\n\\noffload_optimizer\\n\\nTrue\\n\\nallgather_bucket_size\\n\\n5e8\\n\\nreduce_bucket_size\\n\\n5e8\\n\\n),\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nNote\\n\\nWe suggest tuning the allgather_bucket_size parameter and reduce_bucket_size parameter to find optimum parameters based on your model size.\\nThese control how large a buffer we limit the model to using when reducing gradients/gathering updated parameters. Smaller values will result in less memory, but tradeoff with speed.\\n\\nDeepSpeed allocates a reduce buffer size multiplied by 1.5x so take that into consideration when tweaking the parameters.\\n\\nThe strategy sets a reasonable default of 2e8, which should work for most low VRAM GPUs (less than 7GB), allocating roughly 3.6GB of VRAM as buffer. Higher VRAM GPUs should aim for values around 5e8.\\n\\nFor even more speed benefit, DeepSpeed offers an optimized CPU version of ADAM called DeepSpeedCPUAdam to run the offloaded computation, which is faster than the standard PyTorch implementation.\\n\\nimport\\n\\nlightning.pytorch\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\ndeepspeed.ops.adam\\n\\nimport\\n\\nDeepSpeedCPUAdam\\n\\nclass\\n\\nMyModel\\n\\npl\\n\\nLightningModule\\n\\n):\\n\\n...\\n\\ndef\\n\\nconfigure_optimizers\\n\\nself\\n\\n):\\n\\n# DeepSpeedCPUAdam provides 5x to 7x speedup over torch.optim.adam(w)\\n\\nreturn\\n\\nDeepSpeedCPUAdam\\n\\nself\\n\\nparameters\\n\\n())\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_2_offload\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nDeepSpeed ZeRO Stage 3\\n\\nDeepSpeed ZeRO Stage 3 shards the optimizer states, gradients and the model parameters (also optionally activations). Sharding model parameters and activations comes with an increase in distributed communication, however allows you to scale your models massively from one GPU to multiple GPUs.\\nThe DeepSpeed team report the ability to fine-tune models with over 40B parameters on a single GPU and over 2 Trillion parameters on 512 GPUs. For more information we suggest checking the DeepSpeed ZeRO-3 Offload documentation.\\n\\nWe’ve ran benchmarks for all these features and given a simple example of how all these features work in Lightning, which you can see at minGPT.\\n\\nTo reach the highest memory efficiency or model size, you must:\\n\\nUse the DeepSpeed strategy with the stage 3 parameter\\n\\nUse CPU Offloading to offload weights to CPU, plus have a reasonable amount of CPU RAM to offload onto\\n\\nUse DeepSpeed Activation Checkpointing to shard activations\\n\\nBelow we describe how to enable all of these to see benefit. With all these improvements we reached 45 Billion parameters training a GPT model on 8 GPUs with ~1TB of CPU RAM available.\\n\\nAlso please have a look at our DeepSpeed ZeRO Stage 3 Tips which contains a lot of helpful information when configuring your own models.\\n\\nNote\\n\\nWhen saving a model using DeepSpeed and Stage 3, model states and optimizer states will be saved in separate sharded states (based on the world size). See Collating Single File Checkpoint for DeepSpeed ZeRO Stage 3 to obtain a single checkpoint file.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\ndeepspeed.ops.adam\\n\\nimport\\n\\nFusedAdam\\n\\nclass\\n\\nMyModel\\n\\npl\\n\\nLightningModule\\n\\n):\\n\\n...\\n\\ndef\\n\\nconfigure_optimizers\\n\\nself\\n\\n):\\n\\nreturn\\n\\nFusedAdam\\n\\nself\\n\\nparameters\\n\\n())\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_3\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\ntrainer\\n\\ntest\\n\\n()\\n\\ntrainer\\n\\npredict\\n\\n()\\n\\nYou can also use the Lightning Trainer to run predict or evaluate with DeepSpeed once the model has been trained.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nclass\\n\\nMyModel\\n\\npl\\n\\nLightningModule\\n\\n):\\n\\n...\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_3\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\ntest\\n\\nckpt_path\\n\\n\"my_saved_deepspeed_checkpoint.ckpt\"\\n\\nShard Model Instantly to Reduce Initialization Time/Memory\\n\\nWhen instantiating really large models, it is sometimes necessary to shard the model layers instantly.\\n\\nThis is the case if layers may not fit on one single machines CPU or GPU memory, but would fit once sharded across multiple machines.\\nWe expose a hook that layers initialized within the hook will be sharded instantly on a per layer basis, allowing you to instantly shard models.\\n\\nThis reduces the time taken to initialize very large models, as well as ensure we do not run out of memory when instantiating larger models. For more information you can refer to the DeepSpeed docs for Constructing Massive Models.\\n\\nimport\\n\\ntorch.nn\\n\\nas\\n\\nnn\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\ndeepspeed.ops.adam\\n\\nimport\\n\\nFusedAdam\\n\\nclass\\n\\nMyModel\\n\\npl\\n\\nLightningModule\\n\\n):\\n\\n...\\n\\ndef\\n\\nconfigure_sharded_model\\n\\nself\\n\\n):\\n\\n# Created within sharded model context, modules are instantly sharded across processes\\n\\n# as soon as they are made.\\n\\nself\\n\\nblock\\n\\nnn\\n\\nSequential\\n\\nnn\\n\\nLinear\\n\\n32\\n\\n32\\n\\n),\\n\\nnn\\n\\nReLU\\n\\n())\\n\\ndef\\n\\nconfigure_optimizers\\n\\nself\\n\\n):\\n\\nreturn\\n\\nFusedAdam\\n\\nself\\n\\nparameters\\n\\n())\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_3\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\ntrainer\\n\\ntest\\n\\n()\\n\\ntrainer\\n\\npredict\\n\\n()\\n\\nDeepSpeed ZeRO Stage 3 Offload\\n\\nDeepSpeed ZeRO Stage 3 Offloads optimizer state, gradients to the host CPU to reduce memory usage as ZeRO Stage 2 does, however additionally allows you to offload the parameters as well for even more memory saving.\\n\\nNote\\n\\nWhen saving a model using DeepSpeed and Stage 3, model states and optimizer states will be saved in separate sharded states (based on the world size). See Collating Single File Checkpoint for DeepSpeed ZeRO Stage 3 to obtain a single checkpoint file.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDeepSpeedStrategy\\n\\n# Enable CPU Offloading\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_3_offload\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\n# Enable CPU Offloading, and offload parameters to CPU\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDeepSpeedStrategy\\n\\nstage\\n\\noffload_optimizer\\n\\nTrue\\n\\noffload_parameters\\n\\nTrue\\n\\n),\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nDeepSpeed Infinity (NVMe Offloading)\\n\\nAdditionally, DeepSpeed supports offloading to NVMe drives for even larger models, utilizing the large memory space found in NVMes. DeepSpeed reports the ability to fine-tune 1 Trillion+ parameters using NVMe Offloading on one 8 GPU machine. Below shows how to enable this, assuming the NVMe drive is mounted in a directory called /local_nvme.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDeepSpeedStrategy\\n\\n# Enable CPU Offloading\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_3_offload\"\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\n# Enable CPU Offloading, and offload parameters to CPU\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDeepSpeedStrategy\\n\\nstage\\n\\noffload_optimizer\\n\\nTrue\\n\\noffload_parameters\\n\\nTrue\\n\\nremote_device\\n\\n\"nvme\"\\n\\noffload_params_device\\n\\n\"nvme\"\\n\\noffload_optimizer_device\\n\\n\"nvme\"\\n\\nnvme_path\\n\\n\"/local_nvme\"\\n\\n),\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nWhen offloading to NVMe you may notice that the speed is slow. There are parameters that need to be tuned based on the drives that you are using. Running the aio_bench_perf_sweep.py script can help you to find optimum parameters. See the issue for more information on how to parse the information.\\n\\nDeepSpeed Activation Checkpointing\\n\\nActivation checkpointing frees activations from memory as soon as they are not needed during the forward pass.\\nThey are then re-computed for the backwards pass as needed.\\n\\nActivation checkpointing is very useful when you have intermediate layers that produce large activations.\\n\\nThis saves memory when training larger models, however requires using a checkpoint function to run modules as shown below.\\n\\nWarning\\n\\nEnsure to not wrap the entire model with activation checkpointing. This is not the intended usage of activation checkpointing, and will lead to failures as seen in this discussion.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nimport\\n\\ndeepspeed\\n\\nclass\\n\\nMyModel\\n\\nLightningModule\\n\\n):\\n\\n...\\n\\ndef\\n\\n__init__\\n\\nself\\n\\n):\\n\\nsuper\\n\\n()\\n\\n__init__\\n\\n()\\n\\nself\\n\\nblock_1\\n\\nnn\\n\\nSequential\\n\\nnn\\n\\nLinear\\n\\n32\\n\\n32\\n\\n),\\n\\nnn\\n\\nReLU\\n\\n())\\n\\nself\\n\\nblock_2\\n\\ntorch\\n\\nnn\\n\\nLinear\\n\\n32\\n\\ndef\\n\\nforward\\n\\nself\\n\\n):\\n\\n# Use the DeepSpeed checkpointing function instead of calling the module directly\\n\\n# checkpointing self.block_1 means the activations are deleted after use,\\n\\n# and re-calculated during the backward passes\\n\\ndeepspeed\\n\\ncheckpointing\\n\\ncheckpoint\\n\\nself\\n\\nblock_1\\n\\nreturn\\n\\nself\\n\\nblock_2\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDeepSpeedStrategy\\n\\nimport\\n\\ndeepspeed\\n\\nclass\\n\\nMyModel\\n\\npl\\n\\nLightningModule\\n\\n):\\n\\n...\\n\\ndef\\n\\nconfigure_sharded_model\\n\\nself\\n\\n):\\n\\nself\\n\\nblock_1\\n\\nnn\\n\\nSequential\\n\\nnn\\n\\nLinear\\n\\n32\\n\\n32\\n\\n),\\n\\nnn\\n\\nReLU\\n\\n())\\n\\nself\\n\\nblock_2\\n\\ntorch\\n\\nnn\\n\\nLinear\\n\\n32\\n\\ndef\\n\\nforward\\n\\nself\\n\\n):\\n\\n# Use the DeepSpeed checkpointing function instead of calling the module directly\\n\\ndeepspeed\\n\\ncheckpointing\\n\\ncheckpoint\\n\\nself\\n\\nblock_1\\n\\nreturn\\n\\nself\\n\\nblock_2\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\n\"deepspeed_stage_3_offload\"\\n\\nprecision\\n\\n16\\n\\n# Enable CPU Activation Checkpointing\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDeepSpeedStrategy\\n\\nstage\\n\\noffload_optimizer\\n\\nTrue\\n\\n# Enable CPU Offloading\\n\\ncpu_checkpointing\\n\\nTrue\\n\\n# (Optional) offload activations to CPU\\n\\n),\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nDeepSpeed ZeRO Stage 3 Tips\\n\\nHere is some helpful information when setting up DeepSpeed ZeRO Stage 3 with Lightning.\\n\\nIf you’re using Adam or AdamW, ensure to use FusedAdam or DeepSpeedCPUAdam (for CPU Offloading) rather than the default torch optimizers as they come with large speed benefits\\n\\nTreat your GPU/CPU memory as one large pool. In some cases, you may not want to offload certain things (like activations) to provide even more space to offload model parameters\\n\\nWhen offloading to the CPU, make sure to bump up the batch size as GPU memory will be freed\\n\\nWe also support sharded checkpointing. By passing save_full_weights=False to the DeepSpeedStrategy, we’ll save shards of the model which allows you to save extremely large models. However to load the model and run test/validation/predict you must use the Trainer object.\\n\\nCollating Single File Checkpoint for DeepSpeed ZeRO Stage 3\\n\\nAfter training using ZeRO Stage 3, you’ll notice that your checkpoints are a directory of sharded model and optimizer states. If you’d like to collate a single file from the checkpoint directory please use the below command, which handles all the Lightning states additionally when collating the file.\\n\\nfrom\\n\\nlightning.pytorch.utilities.deepspeed\\n\\nimport\\n\\nconvert_zero_checkpoint_to_fp32_state_dict\\n\\n# lightning deepspeed has saved a directory instead of a file\\n\\nsave_path\\n\\n\"lightning_logs/version_0/checkpoints/epoch=0-step=0.ckpt/\"\\n\\noutput_path\\n\\n\"lightning_model.pt\"\\n\\nconvert_zero_checkpoint_to_fp32_state_dict\\n\\nsave_path\\n\\noutput_path\\n\\nWarning\\n\\nThis single file checkpoint does not include the optimizer/lr-scheduler states. This means we cannot restore training via the trainer.fit(ckpt_path=) call. Ensure to keep the sharded checkpoint directory if this is required.\\n\\nCustom DeepSpeed Config\\n\\nIn some cases you may want to define your own DeepSpeed Config, to access all parameters defined. We’ve exposed most of the important parameters, however, there may be debugging parameters to enable. Also, DeepSpeed allows the use of custom DeepSpeed optimizers and schedulers defined within a config file that is supported.\\n\\nNote\\n\\nAll strategy default parameters will be ignored when a config object is passed.\\nAll compatible arguments can be seen in the DeepSpeed docs.\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDeepSpeedStrategy\\n\\ndeepspeed_config\\n\\n\"zero_allow_untested_optimizer\"\\n\\nTrue\\n\\n\"optimizer\"\\n\\n\"type\"\\n\\n\"OneBitAdam\"\\n\\n\"params\"\\n\\n\"lr\"\\n\\n3e-5\\n\\n\"betas\"\\n\\n0.998\\n\\n0.999\\n\\n],\\n\\n\"eps\"\\n\\n1e-5\\n\\n\"weight_decay\"\\n\\n1e-9\\n\\n\"cuda_aware\"\\n\\nTrue\\n\\n},\\n\\n},\\n\\n\"scheduler\"\\n\\n\"type\"\\n\\n\"WarmupLR\"\\n\\n\"params\"\\n\\n\"last_batch_iteration\"\\n\\n\"warmup_min_lr\"\\n\\n\"warmup_max_lr\"\\n\\n3e-5\\n\\n\"warmup_num_steps\"\\n\\n100\\n\\n},\\n\\n},\\n\\n\"zero_optimization\"\\n\\n\"stage\"\\n\\n# Enable Stage 2 ZeRO (Optimizer/Gradient state partitioning)\\n\\n\"offload_optimizer\"\\n\\nTrue\\n\\n# Enable Offloading optimizer state/calculation to the host CPU\\n\\n\"contiguous_gradients\"\\n\\nTrue\\n\\n# Reduce gradient fragmentation.\\n\\n\"overlap_comm\"\\n\\nTrue\\n\\n# Overlap reduce/backward operation of gradients for speed.\\n\\n\"allgather_bucket_size\"\\n\\n2e8\\n\\n# Number of elements to all gather at once.\\n\\n\"reduce_bucket_size\"\\n\\n2e8\\n\\n# Number of elements we reduce/allreduce at once.\\n\\n},\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDeepSpeedStrategy\\n\\nconfig\\n\\ndeepspeed_config\\n\\n),\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nWe support taking the config as a json formatted file:\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDeepSpeedStrategy\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDeepSpeedStrategy\\n\\nconfig\\n\\n\"/path/to/deepspeed_config.json\"\\n\\n),\\n\\nprecision\\n\\n16\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nYou can use also use an environment variable via your PyTorch Lightning script:\\n\\nPL_DEEPSPEED_CONFIG_PATH\\n\\n=/path/to/deepspeed_config.json\\n\\npython\\n\\ntrain.py\\n\\n-strategy\\n\\ndeepspeed\\n\\nDDP Optimizations\\n\\nDDP Static Graph\\n\\nDDP static graph assumes that your model\\nemploys the same set of used/unused parameters in every iteration, so that it can deterministically know the flow of\\ntraining and apply special optimizations during runtime.\\n\\nNote\\n\\nDDP static graph support requires PyTorch>=1.11.0\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDDPStrategy\\n\\ntrainer\\n\\nTrainer\\n\\ndevices\\n\\nstrategy\\n\\nDDPStrategy\\n\\nstatic_graph\\n\\nTrue\\n\\n))\\n\\nWhen Using DDP on a Multi-node Cluster, Set NCCL Parameters\\n\\nNCCL is the NVIDIA Collective Communications Library that is used by PyTorch to handle communication across nodes and GPUs. There are reported benefits in terms of speedups when adjusting NCCL parameters as seen in this issue. In the issue, we see a 30% speed improvement when training the Transformer XLM-RoBERTa and a 15% improvement in training with Detectron2.\\n\\nNCCL parameters can be adjusted via environment variables.\\n\\nNote\\n\\nAWS and GCP already set default values for these on their clusters. This is typically useful for custom cluster setups.\\n\\nNCCL_NSOCKS_PERTHREAD\\n\\nNCCL_SOCKET_NTHREADS\\n\\nNCCL_MIN_NCHANNELS\\n\\nexport\\n\\nNCCL_NSOCKS_PERTHREAD\\n\\nexport\\n\\nNCCL_SOCKET_NTHREADS\\n\\nGradients as Bucket View\\n\\nEnabling gradient_as_bucket_view=True in the DDPStrategy will make gradients views point to different offsets of the allreduce communication buckets. See DistributedDataParallel for more information.\\n\\nThis can reduce peak memory usage and throughput as saved memory will be equal to the total gradient memory + removes the need to copy gradients to the allreduce communication buckets.\\n\\nNote\\n\\nWhen gradient_as_bucket_view=True you cannot call detach_() on gradients. If hitting such errors, please fix it by referring to the zero_grad() function in torch/optim/optimizer.py as a solution (source).\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDDPStrategy\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDDPStrategy\\n\\ngradient_as_bucket_view\\n\\nTrue\\n\\n))\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nDDP Communication Hooks\\n\\nDDP Communication hooks is an interface to control how gradients are communicated across workers, overriding the standard allreduce in DistributedDataParallel. This allows you to enable performance improving communication hooks when using multiple nodes.\\n\\nEnable FP16 Compress Hook for multi-node throughput improvement:\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDDPStrategy\\n\\nfrom\\n\\ntorch.distributed.algorithms.ddp_comm_hooks\\n\\nimport\\n\\ndefault_hooks\\n\\nas\\n\\ndefault\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDDPStrategy\\n\\nddp_comm_hook\\n\\ndefault\\n\\nfp16_compress_hook\\n\\n))\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nEnable PowerSGD for multi-node throughput improvement:\\n\\nNote\\n\\nPowerSGD typically requires extra memory of the same size as the model’s gradients to enable error feedback, which can compensate for biased compressed communication and improve accuracy (source).\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDDPStrategy\\n\\nfrom\\n\\ntorch.distributed.algorithms.ddp_comm_hooks\\n\\nimport\\n\\npowerSGD_hook\\n\\nas\\n\\npowerSGD\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDDPStrategy\\n\\nddp_comm_state\\n\\npowerSGD\\n\\nPowerSGDState\\n\\nprocess_group\\n\\nNone\\n\\nmatrix_approximation_rank\\n\\nstart_powerSGD_iter\\n\\n5000\\n\\n),\\n\\nddp_comm_hook\\n\\npowerSGD\\n\\npowerSGD_hook\\n\\n),\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nCombine hooks for accumulated benefit:\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDDPStrategy\\n\\nfrom\\n\\ntorch.distributed.algorithms.ddp_comm_hooks\\n\\nimport\\n\\ndefault_hooks\\n\\nas\\n\\ndefault\\n\\npowerSGD_hook\\n\\nas\\n\\npowerSGD\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDDPStrategy\\n\\nddp_comm_state\\n\\npowerSGD\\n\\nPowerSGDState\\n\\nprocess_group\\n\\nNone\\n\\nmatrix_approximation_rank\\n\\nstart_powerSGD_iter\\n\\n5000\\n\\n),\\n\\nddp_comm_hook\\n\\npowerSGD\\n\\npowerSGD_hook\\n\\nddp_comm_wrapper\\n\\ndefault\\n\\nfp16_compress_wrapper\\n\\n),\\n\\ntrainer\\n\\nfit\\n\\nmodel\\n\\nWhen using Post-localSGD, you must also pass model_averaging_period to allow for model parameter averaging:\\n\\nfrom\\n\\nlightning.pytorch\\n\\nimport\\n\\nTrainer\\n\\nfrom\\n\\nlightning.pytorch.strategies\\n\\nimport\\n\\nDDPStrategy\\n\\nfrom\\n\\ntorch.distributed.algorithms.ddp_comm_hooks\\n\\nimport\\n\\npost_localSGD_hook\\n\\nas\\n\\npost_localSGD\\n\\nmodel\\n\\nMyModel\\n\\n()\\n\\ntrainer\\n\\nTrainer\\n\\naccelerator\\n\\n\"gpu\"\\n\\ndevices\\n\\nstrategy\\n\\nDDPStrategy\\n\\nddp_comm_state\\n\\npost_localSGD\\n\\nPostLocalSGDState\\n\\nprocess_group\\n\\nNone\\n\\nsubgroup\\n\\nNone\\n\\nstart_localSGD_iter\\n\\n),\\n\\nddp_comm_hook\\n\\npost_localSGD\\n\\npost_localSGD_hook\\n\\nmodel_averaging_period\\n\\n),\\n\\ntrainer\\n\\nfit\\n\\nmodel', metadata={'source': 'pl-docs/Train 1 trillion+ parameter models — PyTorch Lightning 2.1.0dev documentation.html'})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"mixed precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "am",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
